{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"},{"sourceId":13637997,"sourceType":"datasetVersion","datasetId":8666445}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Cell 1\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - REFACTORED\n#\n# Cell 1: Imports & Environment Setup\n#\n# This cell is designed with \"backwards-planning\" (MDMP) based on our\n# 3-Phase (Heuristic, Abstraction, Reasoning) roadmap.\n#\n# We are importing all dependencies for the *entire system* up front\n# to ensure a clean, linear execution flow.\n#\n################################################################################\n\n# --- Core Python Libraries ---\nimport numpy as np\nimport json\nimport time\nimport os\nimport gc\nimport sys\nimport resource\nimport pickle\nimport re\nimport copy\nimport signal\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Any, Callable, Set\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, Counter, deque\nfrom enum import Enum\nfrom itertools import combinations, product\n\n# --- Phase 1: Heuristic Triage & Object Perception ---\n# scipy.ndimage.label is the S-tier replacement for our\n# custom _flood_fill_label. It is C-optimized and robust.\nfrom scipy.ndimage import label as scipy_label\n\n# --- Phase 3: Reasoning Engine (Deep Search) ---\n# Required for the true parallel search in our ReasoningSolver\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n\n# --- Environment & Logging Setup ---\n# Set higher recursion depth for deep symbolic search\nsys.setrecursionlimit(10000)\n\nprint(\"=\"*70)\nprint(\"üåä‚öõÔ∏è LucidOrca Solver: Cell 1 Imports Loaded\")\nprint(f\"  Python Version: {sys.version.split(' ')[0]}\")\nprint(f\"  Numpy Version: {np.__version__}\")\nprint(f\"  Recursion Limit: {sys.getrecursionlimit()}\")\nprint(\"=\"*70)\n#Cell 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:38:58.520144Z","iopub.execute_input":"2025-11-09T12:38:58.520513Z","iopub.status.idle":"2025-11-09T12:38:58.692323Z","shell.execute_reply.started":"2025-11-09T12:38:58.520489Z","shell.execute_reply":"2025-11-09T12:38:58.691128Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 1 Imports Loaded\n  Python Version: 3.11.13\n  Numpy Version: 1.26.4\n  Recursion Limit: 10000\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Cell 2\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 2: Global Configuration & Core Utilities\n#\n# *** v2.0 REBUILD (DSS Core) ***\n# 1. MOVED: `HyperObject` (the \"Noun\") is moved here from Cell 3 to\n#    resolve the NameError.\n# 2. ADDED: The `CognitiveState` dataclass (the \"Live Map\"). This is the\n#    new central data structure for the v2.0 synthesizer.\n#\n################################################################################\n\n# --- 1. Global Configuration ---\n\n@dataclass\nclass ChampionshipConfig:\n    \"\"\"\n    Single source of truth for all solver parameters.\n    This allows for easy tuning and validation.\n    \"\"\"\n    \n    # --- R&D / Diagnostic Mode ---\n    DIAGNOSTIC_RUN: bool = False\n    DIAGNOSTIC_SAMPLE_SIZE: int = 100 # Number of tasks for \"micro-train\"\n    \n    # --- HOTFIX 16 Knobs ---\n    DIAGNOSTIC_MIN_RUNTIME_MINUTES: float = 30.0 # 30-minute minimum\n    PUNT_TASK_BUDGET_SECONDS: float = 60.0       # 1-minute per punt task\n    \n    \n    # --- Time Management (in seconds) ---\n    total_time_budget: float = 28800.0   # 8 hours (Kaggle Hard Limit is 9)\n    submission_buffer: float = 900.0     # 15 min buffer for saving files\n    \n    # --- LTM Training Budget (HOTFIX 11) ---\n    LTM_BUDGET_PERCENT: float = 0.30     # 30% of total_time_budget\n    \n    # --- Time Allocation Ratios for 3-Phase Solving (Inference) ---\n    abstraction_pass_time_ratio: float = 0.30 # 30% of *remaining* time\n    reasoning_pass_time_ratio: float = 0.70 # 70% of *remaining* time\n\n    # --- System & Resource ---\n    parallel_workers: int = 4                # Match Kaggle's 4 CPU cores\n    kaggle_memory_gb: float = 16.0\n    memory_limit_ratio: float = 0.75         # Use 75% of 16GB = 12GB\n    max_memory_bytes: int = int(kaggle_memory_gb * memory_limit_ratio * 1024**3)\n\n    # --- LTM-v4 Solver Behavior Knobs ---\n    \n    # *** HOTFIX 19: \"Ultra-Deep, Hyper-Pruned\" ***\n    MAX_PROGRAM_DEPTH: int = 150 # Increased from 15\n    BEAM_SEARCH_WIDTH: int = 8 # Kept from HOTFIX 18\n    \n    # k-Nearest Neighbors for LTM cache query\n    LTM_CACHE_K: int = 5\n\n\n# Instantiate the global config object\nCONFIG = ChampionshipConfig()\n\nprint(\"=\"*70)\nprint(f\"üåä‚öõÔ∏è LucidOrca Solver: Cell 2 Configuration (v2.0 REBUILD)\")\nif CONFIG.DIAGNOSTIC_RUN:\n    print(\"  *** ‚ö†Ô∏è  DIAGNOSTIC MODE ENABLED ‚ö†Ô∏è ***\")\n    print(f\"  Sample Size: {CONFIG.DIAGNOSTIC_SAMPLE_SIZE} tasks\")\n    print(f\"  Minimum Runtime: {CONFIG.DIAGNOSTIC_MIN_RUNTIME_MINUTES:.0f} minutes\")\nprint(f\"  Punt Task Budget: {CONFIG.PUNT_TASK_BUDGET_SECONDS:.0f} seconds\")\nprint(f\"  Total Time Budget: {CONFIG.total_time_budget / 3600:.2f} hours\")\nprint(f\"  LTM Training Budget: {CONFIG.LTM_BUDGET_PERCENT * 100:.0f}% of Total\")\nprint(f\"  Memory Limit: {CONFIG.max_memory_bytes / 1024**3:.2f} GB\")\nprint(f\"  Program Depth: {CONFIG.MAX_PROGRAM_DEPTH} (Actions) | Beam Width: {CONFIG.BEAM_SEARCH_WIDTH}\")\n\n\n# --- 2. Core Utility Classes ---\n\nclass TimingProfiler:\n    \"\"\"Track timing at every level: task, solver, function, operation\"\"\"\n    def __init__(self):\n        self.timings = defaultdict(list)\n        self.start_times = {}\n        self.call_counts = defaultdict(int)\n\n    def start(self, category: str):\n        self.start_times[category] = time.time()\n\n    def end(self, category: str):\n        if category in self.start_times:\n            duration = time.time() - self.start_times[category]\n            self.timings[category].append(duration)\n            self.call_counts[category] += 1\n            del self.start_times[category]\n            return duration\n        return 0.0\n\n    def get_stats(self, category: str = None):\n        if category:\n            if category in self.timings:\n                times = self.timings[category]\n                if not times: return {'count': 0, 'total': 0, 'mean': 0, 'median': 0, 'min': 0, 'max': 0}\n                return {\n                    'count': len(times),\n                    'total': sum(times),\n                    'mean': np.mean(times),\n                    'median': np.median(times),\n                    'min': min(times),\n                    'max': max(times),\n                }\n            return {}\n        return {cat: self.get_stats(cat) for cat in self.timings.keys()}\n\n    def print_summary(self, top_n: int = 20):\n        print(\"\\n\" + \"=\"*70)\n        print(\"‚è±Ô∏è  DETAILED TIMING BREAKDOWN\")\n        print(\"=\"*70)\n        \n        valid_categories = {k: sum(v) for k, v in self.timings.items() if v}\n        if not valid_categories:\n            print(\"  No timing data recorded.\")\n            print(\"=\"*70)\n            return\n\n        sorted_categories = sorted(\n            valid_categories.keys(),\n            key=lambda k: valid_categories[k],\n            reverse=True\n        )[:top_n]\n        \n        for cat in sorted_categories:\n            stats = self.get_stats(cat)\n            print(f\"  {cat:40s}: {stats['total']:7.2f}s ({stats['count']:4d} calls, \"\n                  f\"avg: {stats['mean']:.3f}s)\")\n        print(\"=\"*70)\n\nprofiler = TimingProfiler()\n\n# --- Metric Logger (HOTFIX 9) ---\n\nclass MetricLogger:\n    \"\"\"A simple, robust CSV logger for our meta-analysis.\"\"\"\n    def __init__(self, filepath: Path):\n        self.filepath = filepath\n        try:\n            self.file_handle = open(self.filepath, 'w')\n            print(f\"  ‚úÖ MetricLogger initialized. Writing to {self.filepath}\")\n        except Exception as e:\n            print(f\"  ‚ùå CRITICAL: MetricLogger FAILED to open file: {e}\")\n            self.file_handle = None\n\n    def write_header(self, columns: List[str]):\n        if not self.file_handle: return\n        try:\n            print(','.join(columns), file=self.file_handle, flush=True)\n        except Exception as e:\n            print(f\"  ‚ùå MetricLogger Error (Header): {e}\")\n\n    def log(self, data: Dict):\n        if not self.file_handle: return\n        try:\n            str_data = [str(data.get(k, \"\")) for k in data['columns_order']]\n            print(','.join(str_data), file=self.file_handle, flush=True)\n        except Exception as e:\n            print(f\"  ‚ùå MetricLogger Error (Log): {e}\")\n    \n    def close(self):\n        if self.file_handle:\n            self.file_handle.close()\n            print(f\"  ‚úÖ MetricLogger closed. Final logs saved to {self.filepath}\")\n\n\n# --- *** v2.0 Dependency Fix: Moved HyperObject here *** ---\n@dataclass\nclass HyperObject:\n    \"\"\"\n    A \"noun\" with advanced features, used by all LTM-v4+ systems.\n    This dataclass is the primary output of the PerceptionEngine.\n    \"\"\"\n    obj_id: int\n    color: int\n    size: int\n    positions: np.ndarray  # (N, 2) array of (r, c) coordinates\n    bbox: Tuple[int, int, int, int] # (min_r, min_c, max_r, max_c)\n    center: Tuple[float, float]     # (mean_r, mean_c)\n    \n    # Advanced features (from LTM-v2)\n    symmetry_score: float = 0.0\n    density: float = 0.0\n    hierarchy_level: int = 0\n    topology: str = \"simple\" # 'simple', 'hollow', etc.\n\n    # --- Add hashing for symbolic_state comparison ---\n    def __hash__(self):\n        # A simple hash based on salient, immutable features\n        return hash((self.obj_id, self.color, self.size, self.bbox))\n    \n    def __eq__(self, other):\n        if not isinstance(other, HyperObject):\n            return False\n        return self.obj_id == other.obj_id\n\nprint(\"  Defined: HyperObject (The AGI's 'Noun') [v2.0]\")\n# --- *** End of HyperObject move *** ---\n\n\n# --- *** NEW v2.0 DSS Core *** ---\n@dataclass\nclass CognitiveState:\n    \"\"\"\n    The central data structure for the v2.0 (DSS) Synthesizer.\n    This replaces the old `BeamEntry` and `ctx` dict. It explicitly\n    decouples the pixel-space \"territory\" from the symbolic-space \"map.\"\n    \n    The Synthesizer will search over this state, and the CWM Primitives\n    will update it incrementally, *eliminating* the N+1 perception bottleneck.\n    \"\"\"\n    program_ast: List[Dict]           # History: The program built so far.\n    grid_state: np.ndarray            # Territory: The raw pixel grid.\n    symbolic_state: List[HyperObject] # Map: The \"live\" list of all objects.\n    \n    # Context: A flexible scratchpad for \"Finder\" ops.\n    # It stores *references* to objects in `symbolic_state`.\n    context: Dict[str, Any] = field(default_factory=dict)\n    \n    # Search metadata\n    cost: float = float('inf')\n    \n    def __hash__(self):\n        # Hash is based on the *pixel state* for fast visited-set checking\n        return hash(self.grid_state.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, CognitiveState):\n            return False\n        # Equality is based on *pixel state*\n        return np.array_equal(self.grid_state, other.grid_state)\n    \n    def __lt__(self, other):\n        # For sorting in the priority queue / beam\n        return self.cost < other.cost\n\nprint(\"  Defined: CognitiveState (The AGI's 'Live Map') [v2.0]\")\n# --- End of v2.0 Change ---\n\n\n# --- 3. Resource & Memory Utilities ---\n\ndef setup_memory_limits():\n    \"\"\"Set memory limits to % of Kaggle's kernel limit\"\"\"\n    try:\n        soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n        target_bytes = CONFIG.max_memory_bytes\n        \n        resource.setrlimit(resource.RLIMIT_AS, (target_bytes, hard))\n        \n        soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n        print(f\"üß† Memory limit set: {soft / (1024**3):.2f} GB\")\n        \n        gc.enable()\n        gc.set_threshold(700, 10, 10)\n        print(f\"‚ôªÔ∏è  Garbage collection: ENABLED (aggressive mode)\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not set memory limit (not on Linux?): {e}\")\n\ndef get_memory_usage() -> dict:\n    \"\"\"Get current memory usage statistics\"\"\"\n    try:\n        usage = resource.getrusage(resource.RUSAGE_SELF)\n        max_rss_gb = usage.ru_maxrss / (1024 * 1024)\n        return {'max_rss_gb': max_rss_gb, 'max_rss_mb': usage.ru_maxrss / 1024}\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Could not get memory usage: {e}\")\n        return {'max_rss_gb': 0, 'max_rss_mb': 0}\n\n\n# --- 4. Difficulty & Time Allocation Heuristics ---\n\ndef estimate_task_difficulty(task: dict) -> float:\n    \"\"\"Estimate task difficulty for curriculum learning (part of Phase 1 Triage).\"\"\"\n    profiler.start(\"estimate_task_difficulty\")\n    examples = task.get('train', [])\n    if not examples:\n        profiler.end(\"estimate_task_difficulty\")\n        return 999.0\n    \n    try:\n        avg_grid_size = np.mean([\n            np.array(ex['input']).size + np.array(ex['output']).size\n            for ex in examples\n        ])\n        grid_complexity = avg_grid_size / 100.0\n\n        all_colors = set()\n        for ex in examples:\n            all_colors.update(np.array(ex['input']).flatten().tolist())\n            all_colors.update(np.array(ex['output']).flatten().tolist())\n        color_complexity = len(all_colors) * 0.5\n\n        num_examples_penalty = 10.0 / (len(examples) + 1)\n\n        shape_changes = sum(\n            1 for ex in examples\n            if np.array(ex['input']).shape != np.array(ex['output']).shape\n        )\n        shape_complexity = shape_changes * 2.0\n\n        size_ratios = []\n        for ex in examples:\n            in_size = np.array(ex['input']).size\n            out_size = np.array(ex['output']).size\n            if in_size > 0:\n                size_ratios.append(abs(out_size / in_size - 1.0))\n        size_change_complexity = np.mean(size_ratios) * 3.0 if size_ratios else 0.0\n\n        difficulty = (\n            grid_complexity +\n            color_complexity +\n            num_examples_penalty +\n            shape_complexity +\n            size_change_complexity\n        )\n        profiler.end(\"estimate_task_difficulty\")\n        return difficulty\n    \n    except Exception as e:\n        profiler.end(\"estimate_task_difficulty\")\n        return 10.0\n\nprint(\"  Core utilities (Profiler, Memory, Difficulty, MetricLogger) defined.\")\nprint(\"=\"*70)\n#Cell 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:39:05.910876Z","iopub.execute_input":"2025-11-09T12:39:05.911359Z","iopub.status.idle":"2025-11-09T12:39:05.951912Z","shell.execute_reply.started":"2025-11-09T12:39:05.911333Z","shell.execute_reply":"2025-11-09T12:39:05.950568Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 2 Configuration (v2.0 REBUILD)\n  Punt Task Budget: 60 seconds\n  Total Time Budget: 8.00 hours\n  LTM Training Budget: 30% of Total\n  Memory Limit: 12.00 GB\n  Program Depth: 150 (Actions) | Beam Width: 8\n  Defined: HyperObject (The AGI's 'Noun') [v2.0]\n  Defined: CognitiveState (The AGI's 'Live Map') [v2.0]\n  Core utilities (Profiler, Memory, Difficulty, MetricLogger) defined.\n======================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Cell 3\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 3: The Perception Engine\n#\n# *** v2.0 REBUILD (DSS Core) ***\n# 1. `HyperObject` has been MOVED to Cell 2 to fix the dependency error.\n# 2. This cell is now *purely* the `PerceptionEngine` (the \"Vision System\")\n#    and the \"Atomic Primitives\" (building blocks).\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"üåä‚öõÔ∏è LucidOrca Solver: Cell 3 (v2.0) Perception Engine\")\n\n# --- 1. The Core \"Noun\" of our AGI ---\n# (MOVED TO CELL 2)\n\n\n# --- 2. The Core \"Vision System\" of our AGI ---\n\nclass PerceptionEngine:\n    \"\"\"\n    This is the core v2.0 \"Vision System.\"\n    Its job is to take a raw grid and transform it into a\n    structured list of HyperObjects.\n    \n    In the v2.0 architecture, this is called *once* per search to\n    create the initial `CognitiveState.symbolic_state`.\n    \"\"\"\n    def __init__(self):\n        # This class is stateless\n        pass\n\n    def analyze(self, grid: np.ndarray) -> List[HyperObject]:\n        \"\"\"\n        Analyzes a grid and returns a list of all found HyperObjects.\n        \"\"\"\n        profiler.start(\"PerceptionEngine.analyze\")\n        \n        # Use scipy.ndimage.label for C-optimized object finding\n        basic_objects = self._extract_basic_objects(grid)\n        if not basic_objects:\n            profiler.end(\"PerceptionEngine.analyze\")\n            return []\n\n        # Compute advanced features for each object\n        hyper_objects = [self._compute_hyper_features(obj, i, grid) \n                         for i, obj in enumerate(basic_objects)]\n        \n        # Compute relational features (e.g., \"is_inside\")\n        self._compute_hierarchy(hyper_objects)\n        \n        profiler.end(\"PerceptionEngine.analyze\")\n        return hyper_objects\n\n    def _extract_basic_objects(self, grid: np.ndarray) -> List[Dict]:\n        \"\"\"\n        Uses scipy.ndimage.label to find all connected components (\"blobs\").\n        \"\"\"\n        objects = []\n        if grid.size == 0:\n            return objects\n        \n        unique_colors = np.unique(grid)\n        for color in unique_colors:\n            if color == 0: continue # Ignore background\n            \n            color_mask = (grid == color)\n            labeled_array, num_features = scipy_label(color_mask)\n\n            for obj_id in range(1, num_features + 1):\n                obj_mask = (labeled_array == obj_id)\n                positions = np.argwhere(obj_mask)\n                if positions.size == 0: continue\n                \n                min_row, min_col = positions.min(axis=0)\n                max_row, max_col = positions.max(axis=0)\n\n                objects.append({\n                    'color': int(color),\n                    'size': len(positions),\n                    'positions': positions,\n                    'mask': obj_mask,\n                    'bbox': (min_row, min_col, max_row, max_col),\n                    'center': (positions[:, 0].mean(), positions[:, 1].mean()),\n                })\n        return objects\n\n    def _compute_hyper_features(self, obj: Dict, obj_id: int, grid: np.ndarray) -> HyperObject:\n        \"\"\"\n        Upgrades a \"basic object\" dict to a \"HyperObject\" dataclass\n        by computing advanced features (density, symmetry, etc.).\n        \"\"\"\n        min_r, min_c, max_r, max_c = obj['bbox']\n        bbox_area = (max_r - min_r + 1) * (max_c - min_c + 1)\n        \n        # Get the snippet of the grid corresponding to the object\n        obj_region = grid[min_r:max_r+1, min_c:max_c+1]\n        obj_mask_local = obj['mask'][min_r:max_r+1, min_c:max_c+1]\n        obj_grid_snippet = obj_region * obj_mask_local\n\n        # Calculate local symmetry\n        symmetry_h = np.mean(obj_grid_snippet == np.fliplr(obj_grid_snippet)) if obj_grid_snippet.size > 0 else 0.0\n        symmetry_v = np.mean(obj_grid_snippet == np.flipud(obj_grid_snippet)) if obj_grid_snippet.size > 0 else 0.0\n        \n        density = obj['size'] / max(bbox_area, 1)\n        \n        return HyperObject(\n            obj_id=obj_id,\n            color=obj['color'],\n            positions=obj['positions'],\n            size=obj['size'],\n            bbox=obj['bbox'],\n            center=obj['center'],\n            symmetry_score=(symmetry_h + symmetry_v) / 2.0,\n            density=density,\n            topology=\"hollow\" if bbox_area > 0 and density < 0.5 and density > 0.1 else \"simple\"\n        )\n\n    def _compute_hierarchy(self, objects: List[HyperObject]):\n        \"\"\"\n        Calculates relational features, like \"which object is inside which\".\n        Modifies objects in-place.\n        \"\"\"\n        for i, obj_i in enumerate(objects):\n            level = 0\n            for j, obj_j in enumerate(objects):\n                if i == j: continue\n                \n                # Is obj_i's center inside obj_j's bbox?\n                min_r, min_c, max_r, max_c = obj_j.bbox\n                cy, cx = obj_i.center\n                if min_r < cy < max_r and min_c < cx < max_c:\n                    # More advanced check: is it *truly* contained?\n                    # For now, a simple bbox check is a good heuristic.\n                    level += 1\n            obj_i.hierarchy_level = level\n\nprint(\"  Defined: PerceptionEngine (Core v2.0 'Vision' System)\")\n\n\n# --- 3. Atomic Primitive Dictionaries (\"The Building Blocks\") ---\n# (Unchanged)\n\n# Geometric primitives\nATOMIC_PRIMITIVES_GEOMETRIC = {\n    'identity': lambda g: g,\n    'rot90': lambda g: np.rot90(g, 1),\n    'rot180': lambda g: np.rot90(g, 2),\n    'rot270': lambda g: np.rot90(g, 3),\n    'flip_h': lambda g: np.fliplr(g), # Flip horizontal (left-right)\n    'flip_v': lambda g: np.flipud(g), # Flip vertical (up-down)\n    'transpose': lambda g: g.T,\n    'anti_transpose': lambda g: np.rot90(g.T, 2),\n}\n\n# Scaling primitives\nATOMIC_PRIMITIVES_SCALING = {\n    'tile_2x2': lambda g: np.tile(g, (2, 2)),\n    'tile_2x1': lambda g: np.tile(g, (2, 1)),\n    'tile_1x2': lambda g: np.tile(g, (1, 2)),\n    'tile_3x3': lambda g: np.tile(g, (3, 3)),\n}\n\n# Color primitives\nATOMIC_PRIMITIVES_COLOR = {\n    'invert_colors_mod10': lambda g: (9 - g) % 10,\n    'increment_colors_mod10': lambda g: (g + 1) % 10,\n    'decrement_colors_mod10': lambda g: (g - 1) % 10,\n    'mask_nonzero': lambda g: (g > 0).astype(int),\n    'extract_color_1': lambda g: (g == 1).astype(int),\n    'extract_color_2': lambda g: (g == 2).astype(int),\n    'extract_color_3': lambda g: (g == 3).astype(int),\n    'extract_color_4': lambda g: (g == 4).astype(int),\n    'extract_color_5': lambda g: (g == 5).astype(int),\n    'extract_color_6': lambda g: (g == 6).astype(int),\n    'extract_color_7': lambda g: (g == 7).astype(int),\n    'extract_color_8': lambda g: (g == 8).astype(int),\n}\n\n# Spatial & Morphological primitives\nATOMIC_PRIMITIVES_SPATIAL = {\n    'center_crop_1px': lambda g: g[1:-1, 1:-1] if g.shape[0] > 2 and g.shape[1] > 2 else g,\n    'border_pad_1px_zero': lambda g: np.pad(g, 1, mode='constant', constant_values=0),\n    'extract_edges': lambda g: (np.abs(g - np.roll(g, 1, axis=0)) + \n                                np.abs(g - np.roll(g, 1, axis=1))) > 0,\n}\n\n# Aggregation/Value primitives (output grid is a single value)\nATOMIC_PRIMITIVES_AGGREGATION = {\n    'count_unique_colors': lambda g: np.array([[len(np.unique(g[g > 0]))]]),\\\n    'count_pixels': lambda g: np.array([[np.sum(g > 0)]]),\\\n    'get_max_color': lambda g: np.array([[np.max(g)]]),\n}\n\n# All \"atomic\" primitives combined\nALL_ATOMIC_PRIMITIVES = {\n    **ATOMIC_PRIMITIVES_GEOMETRIC,\n    **ATOMIC_PRIMITIVES_SCALING,\n    **ATOMIC_PRIMITIVES_COLOR,\n    **ATOMIC_PRIMITIVES_SPATIAL,\n    **ATOMIC_PRIMITIVES_AGGREGATION,\n}\n\n# Pre-compute the list of (name, function) tuples for fast iteration\natomic_primitives_to_test = list(ALL_ATOMIC_PRIMITIVES.items())\n\nprint(f\"  Defined: {len(ALL_ATOMIC_PRIMITIVES)} 'Atomic Primitives' (Building Blocks)\")\nprint(\"=\"*70)\n#Cell 3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:39:28.200754Z","iopub.execute_input":"2025-11-09T12:39:28.201080Z","iopub.status.idle":"2025-11-09T12:39:28.226492Z","shell.execute_reply.started":"2025-11-09T12:39:28.201057Z","shell.execute_reply":"2025-11-09T12:39:28.225427Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 3 (v2.0) Perception Engine\n  Defined: PerceptionEngine (Core v2.0 'Vision' System)\n  Defined: 30 'Atomic Primitives' (Building Blocks)\n======================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Cell 4\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 4: The Causal World Model (CWM) Primitives\n#\n# *** v2.0 REBUILD (CWM Core) ***\n# 1. This class is no longer just an \"ALU\". It is now a \"Causal World Model.\"\n# 2. All primitives are refactored to operate on `CognitiveState` (from Cell 2).\n# 3. \"Finder\" ops (e.g., `find_objects`) are now \"Selector\" ops. They take\n#    the `symbolic_state` and *filter* it, placing the result in `state.context`.\n#    They are near-instantaneous and DO NOT re-perceive the grid.\n# 4. \"Transformer\" ops (e.g., `recolor`) are now \"Causal\" ops. They\n#    take a state and target objects, then return a *new state* where\n#    *both* the `grid_state` and `symbolic_state` are updated.\n# 5. This architecture *completely eliminates* the N+1 perception bottleneck.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"üåä‚öõÔ∏è LucidOrca Solver: Cell 4 (v2.0) Causal World Model\")\n\n# --- 1. The v2.0 \"Causal Instruction Set\" ---\n\nclass MetaPrimitives:\n    \"\"\"\n    Container for our v2.0 \"Causal World Model\" (CWM) instruction set.\n    \n    Each function takes a `CognitiveState` and parameters, then returns\n    a *new* `CognitiveState` that reflects the causal consequences of the\n    action on both the pixel \"territory\" and the symbolic \"map\".\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        \"\"\"\n        The CWM *only* uses the PerceptionEngine for one-off utilities\n        (like `_bresenham_line`) or for complex ops that *must*\n        re-perceive (like a future 'merge_objects' op).\n        It NO LONGER uses it for basic state updates.\n        \"\"\"\n        self.perception_engine = perception_engine\n        print(\"  MetaPrimitives (v2.0 'Causal World Model') initialized.\")\n\n    # --- \"Selector\" Primitives (The \"Nouns\") ---\n    # These ops are now symbolic filters. They are computationally trivial.\n    \n    def find_objects(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Filters `state.symbolic_state` based on params.\n        Stores *references* to the objects in `state.context['last_result']`.\n        Returns a *new state* with an updated context.\n        \"\"\"\n        profiler.start(\"v2.Primitive.find_objects\")\n        \n        # Create a new state by copying context (symbols/grid are unchanged)\n        new_state = copy.deepcopy(state) \n        \n        all_objects = new_state.symbolic_state\n        filtered_objects = all_objects\n        \n        if 'color' in params:\n            filtered_objects = [o for o in filtered_objects if o.color == params['color']]\n        if 'size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size == params['size']]\n        if 'min_size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size >= params['min_size']]\n        if 'max_size' in params:\n            filtered_objects = [o for o in filtered_objects if o.size <= params['max_size']]\n        if 'topology' in params:\n            filtered_objects = [o for o in filtered_objects if o.topology == params['topology']]\n        if 'hierarchy' in params:\n            filtered_objects = [o for o in filtered_objects if o.hierarchy_level == params['hierarchy']]\n            \n        new_state.context['last_result'] = filtered_objects\n        profiler.end(\"v2.Primitive.find_objects\")\n        return new_state # Return new state with updated context\n\n    def get_largest(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"Takes a list of objects from `state.context` and returns only the largest.\"\"\"\n        new_state = copy.deepcopy(state)\n        \n        target_key = params.get('target', 'last_result')\n        objects = new_state.context.get(target_key, [])\n        if not objects or not isinstance(objects, list):\n            new_state.context['last_result'] = []\n            return new_state\n            \n        max_size = max(o.size for o in objects)\n        new_state.context['last_result'] = [o for o in objects if o.size == max_size]\n        return new_state\n\n    def get_smallest(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"Takes a list of objects from `state.context` and returns only the smallest.\"\"\"\n        new_state = copy.deepcopy(state)\n        \n        target_key = params.get('target', 'last_result')\n        objects = new_state.context.get(target_key, [])\n        if not objects or not isinstance(objects, list):\n            new_state.context['last_result'] = []\n            return new_state\n            \n        min_size = min(o.size for o in objects)\n        new_state.context['last_result'] = [o for o in objects if o.size == min_size]\n        return new_state\n\n\n    # --- \"Causal Transformer\" Primitives (The \"Verbs\") ---\n    # These ops update *both* pixel and symbol states.\n    \n    def move(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Causal Op: Moves objects from `state.context` by a `delta` (dr, dc).\n        Returns a *new state* with updated grid and symbolic_state.\n        \"\"\"\n        profiler.start(\"v2.Primitive.move\")\n        target_key = params.get('target', 'last_result')\n        delta = params.get('delta', (0, 0))\n        dr, dc = delta\n        \n        target_objects = state.context.get(target_key, [])\n        if not target_objects or not isinstance(target_objects, list):\n            profiler.end(\"v2.Primitive.move\")\n            return state # Return original state\n            \n        new_grid = state.grid_state.copy()\n        rows, cols = new_grid.shape\n        \n        # 1. Create a deep copy of the \"live map\"\n        new_symbol_map = {obj.obj_id: copy.deepcopy(obj) for obj in state.symbolic_state}\n        \n        # 2. Update Pixel State (in two passes to handle overlaps)\n        # Pass 1: Erase old positions\n        for target in target_objects:\n            for r, c in target.positions:\n                new_grid[r, c] = 0\n                \n        # Pass 2: Draw new positions\n        for target in target_objects:\n            new_positions = []\n            for r, c in target.positions:\n                new_r, new_c = r + dr, c + dc\n                if 0 <= new_r < rows and 0 <= new_c < cols:\n                    new_grid[new_r, new_c] = target.color\n                    new_positions.append((new_r, new_c))\n            \n            # 3. Update Symbolic State (the \"live map\")\n            if target.obj_id in new_symbol_map:\n                obj_ref = new_symbol_map[target.obj_id]\n                obj_ref.positions = np.array(new_positions)\n                if obj_ref.positions.size > 0:\n                    min_r, min_c = obj_ref.positions.min(axis=0)\n                    max_r, max_c = obj_ref.positions.max(axis=0)\n                    obj_ref.bbox = (min_r, min_c, max_r, max_c)\n                    obj_ref.center = (obj_ref.positions[:, 0].mean(), obj_ref.positions[:, 1].mean())\n                else: # Object moved off-grid\n                    obj_ref.bbox = (0,0,0,0)\n                    obj_ref.center = (0,0)\n\n        profiler.end(\"v2.Primitive.move\")\n        return CognitiveState(\n            program_ast=state.program_ast, # Will be updated by Synthesizer\n            grid_state=new_grid,\n            symbolic_state=list(new_symbol_map.values()),\n            context=copy.deepcopy(state.context),\n            cost=float('inf')\n        )\n\n    def delete(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Causal Op: Deletes objects from `state.context`.\n        Returns a *new state* with updated grid and symbolic_state.\n        \"\"\"\n        profiler.start(\"v2.Primitive.delete\")\n        target_key = params.get('target', 'last_result')\n        \n        target_objects = state.context.get(target_key, [])\n        if not target_objects or not isinstance(target_objects, list):\n            profiler.end(\"v2.Primitive.delete\")\n            return state\n            \n        new_grid = state.grid_state.copy()\n        \n        # 1. Create a deep copy of the \"live map\"\n        new_symbol_map = {obj.obj_id: copy.deepcopy(obj) for obj in state.symbolic_state}\n        target_ids_to_delete = {obj.obj_id for obj in target_objects}\n        \n        for target in target_objects:\n            # 2. Update Pixel State\n            for r, c in target.positions:\n                new_grid[r, c] = 0\n            \n            # 3. Update Symbolic State (the \"live map\")\n            if target.obj_id in new_symbol_map:\n                del new_symbol_map[target.obj_id]\n        \n        profiler.end(\"v2.Primitive.delete\")\n        return CognitiveState(\n            program_ast=state.program_ast,\n            grid_state=new_grid,\n            symbolic_state=list(new_symbol_map.values()), # Now shorter\n            context=copy.deepcopy(state.context),\n            cost=float('inf')\n        )\n\n    def recolor(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Causal Op: Recolors objects from `state.context` to a `new_color`.\n        Returns a *new state* with updated grid and symbolic_state.\n        \"\"\"\n        profiler.start(\"v2.Primitive.recolor\")\n        target_key = params.get('target', 'last_result')\n        new_color = params.get('color', 0)\n        \n        target_objects = state.context.get(target_key, [])\n        if not target_objects or not isinstance(target_objects, list):\n            profiler.end(\"v2.Primitive.recolor\")\n            return state\n            \n        new_grid = state.grid_state.copy()\n        \n        # 1. Create a deep copy of the \"live map\"\n        new_symbol_map = {obj.obj_id: copy.deepcopy(obj) for obj in state.symbolic_state}\n        \n        for target in target_objects:\n            # 2. Update Pixel State\n            for r, c in target.positions:\n                new_grid[r, c] = new_color\n            \n            # 3. Update Symbolic State (the \"live map\")\n            if target.obj_id in new_symbol_map:\n                new_symbol_map[target.obj_id].color = new_color\n        \n        profiler.end(\"v2.Primitive.recolor\")\n        return CognitiveState(\n            program_ast=state.program_ast,\n            grid_state=new_grid,\n            symbolic_state=list(new_symbol_map.values()),\n            context=copy.deepcopy(state.context),\n            cost=float('inf')\n        )\n    \n    def copy_objects(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Causal Op: Copies objects from `state.context` by a `delta`.\n        Returns a *new state* with updated grid and *new objects*\n        added to the symbolic_state.\n        \"\"\"\n        profiler.start(\"v2.Primitive.copy_objects\")\n        target_key = params.get('target', 'last_result')\n        delta = params.get('delta', (0, 0))\n        dr, dc = delta\n        \n        target_objects = state.context.get(target_key, [])\n        if not target_objects or not isinstance(target_objects, list):\n            profiler.end(\"v2.Primitive.copy_objects\")\n            return state\n            \n        new_grid = state.grid_state.copy()\n        rows, cols = new_grid.shape\n        \n        # 1. Create a deep copy of the \"live map\"\n        new_symbol_map = {obj.obj_id: copy.deepcopy(obj) for obj in state.symbolic_state}\n        \n        # Get the next available object ID\n        next_obj_id = max(new_symbol_map.keys()) + 1 if new_symbol_map else 0\n        \n        new_objects_created = []\n        \n        for target in target_objects:\n            new_positions = []\n            for r, c in target.positions:\n                new_r, new_c = r + dr, c + dc\n                if 0 <= new_r < rows and 0 <= new_c < cols:\n                    # 2. Update Pixel State\n                    new_grid[new_r, new_c] = target.color\n                    new_positions.append((new_r, new_c))\n            \n            if not new_positions:\n                continue # Copy was fully off-grid\n            \n            # 3. Update Symbolic State (Create *new* object)\n            new_pos_arr = np.array(new_positions)\n            min_r, min_c = new_pos_arr.min(axis=0)\n            max_r, max_c = new_pos_arr.max(axis=0)\n            \n            # Deepcopy the original object's features\n            new_obj = copy.deepcopy(target) \n            \n            # Update with new state\n            new_obj.obj_id = next_obj_id\n            new_obj.positions = new_pos_arr\n            new_obj.bbox = (min_r, min_c, max_r, max_c)\n            new_obj.center = (new_pos_arr[:, 0].mean(), new_pos_arr[:, 1].mean())\n            \n            new_symbol_map[next_obj_id] = new_obj\n            new_objects_created.append(new_obj)\n            next_obj_id += 1\n        \n        # Update context to point to the *newly created* objects\n        new_context = copy.deepcopy(state.context)\n        new_context['last_result'] = new_objects_created\n        \n        profiler.end(\"v2.Primitive.copy_objects\")\n        return CognitiveState(\n            program_ast=state.program_ast,\n            grid_state=new_grid,\n            symbolic_state=list(new_symbol_map.values()),\n            context=new_context,\n            cost=float('inf')\n        )\n    \n    # --- Drawing Primitive (Utility) ---\n    \n    def _bresenham_line(self, grid: np.ndarray, p1: Tuple[int, int], \n                        p2: Tuple[int, int], color: int):\n        \"\"\" Internal helper. Draws a line on `grid` in-place. \"\"\"\n        x0, y0 = p1[1], p1[0] # (col, row)\n        x1, y1 = p2[1], p2[0] # (col, row)\n        rows, cols = grid.shape\n\n        dx = abs(x1 - x0)\n        dy = -abs(y1 - y0)\n        sx = 1 if x0 < x1 else -1\n        sy = 1 if y0 < y1 else -1\n        err = dx + dy\n        \n        while True:\n            if 0 <= y0 < rows and 0 <= x0 < cols:\n                grid[y0, x0] = color\n            if x0 == x1 and y0 == y1:\n                break\n            e2 = 2 * err\n            if e2 >= dy:\n                err += dy\n                x0 += sx\n            if e2 <= dx:\n                err += dx\n                y0 += sy\n\n    def draw_path(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Causal Op: Draws lines between objects from `state.context`.\n        \n        NOTE: This op is \"symbolically blind\" for now. It updates the\n        pixel grid but does *not* create a new \"line\" HyperObject, as\n        that would require re-running perception. This is a v2.1 goal.\n        \"\"\"\n        profiler.start(\"v2.Primitive.draw_path\")\n        \n        target_key = params.get('target', 'all_objects')\n        color = params.get('color', 1)\n        if color == 0: color = 1\n            \n        target_objects = state.context.get(target_key, [])\n        if not target_objects or not isinstance(target_objects, list) or len(target_objects) < 2:\n            profiler.end(\"v2.Primitive.draw_path\")\n            return state\n            \n        new_grid = state.grid_state.copy()\n        \n        # Sort objects top-to-bottom, left-to-right\n        sorted_objects = sorted(target_objects, key=lambda o: (o.center[0], o.center[1]))\n        \n        for i in range(len(sorted_objects) - 1):\n            p1 = (int(round(sorted_objects[i].center[0])), int(round(sorted_objects[i].center[1])))\n            p2 = (int(round(sorted_objects[i+1].center[0])), int(round(sorted_objects[i+1].center[1])))\n            \n            # 2. Update Pixel State\n            self._bresenham_line(new_grid, p1, p2, color)\n            \n        # 3. Update Symbolic State (NO-OP for v2.0)\n        # We accept temporary state-desync for this complex op.\n        # The new grid hash will be different, forcing a new search path.\n        # The *next* op (e.g., a \"find\") will be based on a stale symbolic map.\n        # This is an acceptable trade-off to avoid re-running perception.\n        \n        profiler.end(\"v2.Primitive.draw_path\")\n        return CognitiveState(\n            program_ast=state.program_ast,\n            grid_state=new_grid,\n            symbolic_state=copy.deepcopy(state.symbolic_state), # Pass old map\n            context=copy.deepcopy(state.context),\n            cost=float('inf')\n        )\n\nprint(\"  Defined: MetaPrimitives (The v2.0 'Causal World Model')\")\nprint(\"=\"*70)\n#Cell 4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:39:46.501497Z","iopub.execute_input":"2025-11-09T12:39:46.501808Z","iopub.status.idle":"2025-11-09T12:39:46.546870Z","shell.execute_reply.started":"2025-11-09T12:39:46.501786Z","shell.execute_reply":"2025-11-09T12:39:46.545669Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 4 (v2.0) Causal World Model\n  Defined: MetaPrimitives (The v2.0 'Causal World Model')\n======================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#Cell 5\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - (v2.0 COMPATIBLE)\n#\n# Cell 5: Cognitive Fingerprinting & Triage Engine\n#\n# *** v2.0 REBUILD: NO CHANGES REQUIRED ***\n# This cell is a mandatory dependency for Cells 6, 7, and 9.\n# It defines:\n#   1. `TaskProfile`: The \"Task Passport\"\n#   2. `VisionModelEncoder`: The Grid-to-Vector engine\n#   3. `TaskFingerprinter`: The \"Probe\" generator\n#   4. `TaskAnalyzer`: The Triage Engine\n#\n# It correctly uses the `PerceptionEngine` from Cell 3 and provides\n# the necessary classes for the new v2.0 components.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"üåä‚öõÔ∏è LucidOrca Solver: Cell 5 (v2.0) Fingerprinting & Triage\")\n\n# --- 1. The \"Task Passport\" Dataclass ---\n\n@dataclass\nclass TaskProfile:\n    \"\"\"\n    A \"passport\" for each task, generated by the TaskAnalyzer in Phase 1.\n    It holds all metadata needed for solving, including the LTM-v4 \"probe\".\n    \"\"\"\n    task_id: str\n    difficulty_tier: str  # 'easy', 'medium', 'hard'\n    basin: str            # 'rotation', 'color_mapping', 'scaling', 'unknown'\n    \n    # --- The LTM-v4 \"Probe\" ---\n    delta_fingerprint: Optional[np.ndarray] = None\n    \n    # --- HOTFIX 11: Store raw score for sorting ---\n    difficulty_score: float = 0.0\n\nprint(\"  Defined: TaskProfile (The AGI's 'Task Passport')\")\n\n\n# --- 2. The Core Grid-to-Vector Encoder ---\n\nclass VisionModelEncoder:\n    \"\"\"\n    An \"unrolled\" vision model. It extracts statistical and object-based\n    features from a single grid and encodes them into a normalized\n    1D numpy vector (the \"fingerprint\").\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        \"\"\"\n        The encoder *requires* the PerceptionEngine from Cell 3 to \"see\"\n        objects and calculate object-based statistics.\n        \"\"\"\n        self.perception_engine = perception_engine\n        self.zero_vector = np.zeros(self.get_vector_dim())\n        print(f\"  VisionModelEncoder (Grid-to-Vector Engine) initialized. Fingerprint dim: {self.get_vector_dim()}\")\n\n    def get_vector_dim(self) -> int:\n        \"\"\"Returns the total dimension of our feature vector.\"\"\"\n        # shape(2) + color_hist(10) + obj_stats(4) + symmetry(3) + \n        # patterns(4) + layout(4) + complexity(1) = 28 dimensions\n        return 28\n\n    def encode_grid_to_vector(self, grid: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Encodes a single grid into a normalized 1D numpy vector (\"fingerprint\").\n        \"\"\"\n        profiler.start(\"VisionModelEncoder.encode_grid_to_vector\")\n        \n        if grid.size == 0:\n            profiler.end(\"VisionModelEncoder.encode_grid_to_vector\")\n            return self.zero_vector \n            \n        edge_density_val = self._compute_edge_density(grid)\n        \n        objects = self.perception_engine.analyze(grid)\n        obj_stats = self._compute_object_stats(objects, grid.size)\n        \n        shape_norm = np.array([grid.shape[0], grid.shape[1]]) / 30.0\n        color_hist = np.bincount(grid.flatten(), minlength=10) / max(1.0, grid.size)\n        edge_density = np.array([edge_density_val])\n        symmetry_vec = self._detect_symmetry(grid, as_vector=True)\n        patterns_vec = self._match_patterns(grid, as_vector=True)\n        layout_vec = self._analyze_layout(grid, as_vector=True)\n        complexity = np.array([self._compute_complexity(grid, edge_density_val, obj_stats[0])])\n\n        vector = np.concatenate([\n            shape_norm,      # 2 dims\n            color_hist,      # 10 dims\n            obj_stats,       # 4 dims\n            symmetry_vec,    # 3 dims\n            patterns_vec,    # 4 dims\n            layout_vec,      # 4 dims\n            complexity       # 1 dim\n        ])\n        \n        profiler.end(\"VisionModelEncoder.encode_grid_to_vector\")\n        return np.nan_to_num(vector, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # --- ENCODER HELPER METHODS ---\n\n    def _compute_object_stats(self, objects: List[HyperObject], grid_size: int) -> np.ndarray:\n        \"\"\"Calculates normalized statistics about the object population.\"\"\"\n        if not objects:\n            return np.zeros(4) # count, avg_size, avg_density, num_colors\n            \n        obj_count = len(objects)\n        avg_size = np.mean([o.size for o in objects])\n        avg_density = np.mean([o.density for o in objects])\n        num_colors = len(set(o.color for o in objects))\n        \n        norm_obj_count = min(obj_count / 100.0, 1.0)\n        norm_avg_size = min(avg_size / (grid_size + 1e-6), 1.0)\n        norm_avg_density = avg_density\n        norm_num_colors = min(num_colors / 10.0, 1.0)\n        \n        return np.array([norm_obj_count, norm_avg_size, norm_avg_density, norm_num_colors])\n\n    @staticmethod\n    def _compute_edge_density(grid: np.ndarray) -> float:\n        if grid.size == 0: return 0.0\n        h_edges = np.sum(np.abs(np.diff(grid, axis=0)))\n        v_edges = np.sum(np.abs(np.diff(grid, axis=1)))\n        return (h_edges + v_edges) / max(grid.size * 9, 1) \n\n    @staticmethod\n    def _detect_symmetry(grid: np.ndarray, as_vector: bool = False) -> Any:\n        if grid.size == 0: \n            return np.zeros(3) if as_vector else []\n            \n        symmetries = []\n        is_h, is_v, is_d = 0.0, 0.0, 0.0\n        try:\n            if np.array_equal(grid, np.flipud(grid)): \n                symmetries.append('horizontal'); is_h = 1.0\n            if np.array_equal(grid, np.fliplr(grid)): \n                symmetries.append('vertical'); is_v = 1.0\n            if grid.shape[0] == grid.shape[1]:\n                if np.array_equal(grid, grid.T): \n                    symmetries.append('diag_main'); is_d = 1.0\n        except Exception: pass\n        \n        return np.array([is_h, is_v, is_d]) if as_vector else symmetries\n\n    def _match_patterns(self, grid: np.ndarray, as_vector: bool = False) -> Any:\n        if grid.size == 0:\n            return np.zeros(4) if as_vector else []\n            \n        patterns = []\n        is_stripe, is_grid_p, is_binary, is_sparse = 0.0, 0.0, 0.0, 0.0\n        try:\n            if self._is_stripe_pattern(grid): \n                patterns.append('stripe'); is_stripe = 1.0\n            if self._is_grid_pattern(grid): \n                patterns.append('grid'); is_grid_p = 1.0\n            if len(np.unique(grid)) <= 2: \n                patterns.append('binary_color'); is_binary = 1.0\n            if np.sum(grid == 0) > grid.size * 0.8: \n                patterns.append('sparse'); is_sparse = 1.0\n        except Exception: pass\n        \n        return np.array([is_stripe, is_grid_p, is_binary, is_sparse]) if as_vector else patterns\n\n    @staticmethod\n    def _is_stripe_pattern(grid: np.ndarray) -> bool:\n        if grid.shape[0] < 2 or grid.shape[1] < 2: return False\n        h_stripe = all(len(np.unique(row)) == 1 for row in grid)\n        v_stripe = all(len(np.unique(col)) == 1 for col in grid.T)\n        return h_stripe or v_stripe\n\n    @staticmethod\n    def _is_grid_pattern(grid: np.ndarray) -> bool:\n        if grid.size < 4: return False\n        nonzero = np.argwhere(grid != 0)\n        rows, cols = nonzero[:, 0], nonzero[:, 1]\n        row_diffs, col_diffs = np.diff(np.sort(np.unique(rows))), np.diff(np.sort(np.unique(cols)))\n        row_regular = (len(row_diffs) > 0) and (len(np.unique(row_diffs)) == 1)\n        col_regular = (len(col_diffs) > 0) and (len(np.unique(col_diffs)) == 1)\n        return row_regular and col_regular\n\n    @staticmethod\n    def _analyze_layout(grid: np.ndarray, as_vector: bool = False) -> Any:\n        is_empty, is_centered, is_scattered, is_distributed = 0.0, 0.0, 0.0, 1.0\n        layout_str = 'distributed'\n\n        if grid.size == 0: \n            is_empty, is_distributed = 1.0, 0.0\n            layout_str = 'empty'\n        else:\n            nonzero = np.argwhere(grid != 0)\n            if len(nonzero) == 0:\n                is_empty, is_distributed = 1.0, 0.0\n                layout_str = 'empty'\n            else:\n                centroid = nonzero.mean(axis=0)\n                center_h, center_w = (grid.shape[0] - 1) / 2, (grid.shape[1] - 1) / 2\n                dist = np.linalg.norm(centroid - np.array([center_h, center_w]))\n                spread = nonzero.std(axis=0).mean()\n                \n                max_dim = max(grid.shape[0], grid.shape[1], 1.0)\n                \n                if dist < max_dim * 0.2 and spread < max_dim * 0.3:\n                    is_centered, is_distributed = 1.0, 0.0\n                    layout_str = 'centered'\n                elif spread > max_dim * 0.4:\n                    is_scattered, is_distributed = 1.0, 0.0\n                    layout_str = 'scattered'\n\n        return np.array([is_empty, is_centered, is_scattered, is_distributed]) if as_vector else layout_str\n\n    @staticmethod\n    def _compute_complexity(grid: np.ndarray, edge_density: float, obj_count: float) -> float:\n        \"\"\"Computes a single 'complexity' score [0, 1].\"\"\"\n        if grid.size == 0: return 0.0\n        \n        flat = grid.flatten()\n        _, counts = np.unique(flat, return_counts=True)\n        probs = counts / len(flat)\n        entropy = -np.sum(probs * np.log2(probs + 1e-10))\n        max_entropy = np.log2(len(counts)) if len(counts) > 1 else 1\n        entropy_normalized = entropy / max(1, max_entropy)\n        \n        return (edge_density * 0.3) + (obj_count * 0.3) + (entropy_normalized * 0.4)\n\nprint(\"  Defined: VisionModelEncoder (Grid-to-Vector Engine)\")\n\n\n# --- 3. The Task-to-Delta-Vector Encoder ---\n\nclass TaskFingerprinter:\n    \"\"\"\n    Computes the \"delta-fingerprint\" for an entire task.\n    This vector represents the *abstract transformation* of the task\n    (i.e., v_output - v_input).\n    \"\"\"\n    def __init__(self, vision_encoder: VisionModelEncoder):\n        self.encoder = vision_encoder\n        self.zero_vector = self.encoder.zero_vector\n        print(\"  TaskFingerprinter (Task-to-Delta-Vector Engine) initialized.\")\n\n    def fingerprint(self, task: Dict) -> np.ndarray:\n        \"\"\"\n        Computes the average delta-vector (v_output - v_input)\n        across all training examples for a task.\n        \"\"\"\n        profiler.start(\"TaskFingerprinter.fingerprint\")\n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"TaskFingerprinter.fingerprint\")\n            return self.zero_vector\n\n        delta_vectors = []\n        for ex in examples:\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                \n                v_in = self.encoder.encode_grid_to_vector(inp_grid)\n                v_out = self.encoder.encode_grid_to_vector(out_grid)\n                \n                v_delta = v_out - v_in\n                delta_vectors.append(v_delta)\n            \n            except Exception:\n                continue \n        \n        if not delta_vectors:\n            profiler.end(\"TaskFingerprinter.fingerprint\")\n            return self.zero_vector\n            \n        mean_delta = np.mean(delta_vectors, axis=0)\n        profiler.end(\"TaskFingerprinter.fingerprint\")\n        return mean_delta\n\nprint(\"  Defined: TaskFingerprinter (The 'Probe' Generator)\")\n\n\n# --- 4. The Phase 1 Triage Engine ---\n\nclass TaskAnalyzer:\n    \"\"\"\n    This is the complete Phase 1 \"Heuristic Triage\" engine.\n    Its job is to run a *fast, non-solving* analysis on every task\n    to generate a TaskProfile.\n    \"\"\"\n    def __init__(self, perception_engine: PerceptionEngine):\n        self.encoder = VisionModelEncoder(perception_engine)\n        self.fingerprinter = TaskFingerprinter(self.encoder)\n        print(\"  TaskAnalyzer (Phase 1 Triage Engine) initialized.\")\n\n    def analyze(self, task: Dict, task_id: str) -> TaskProfile:\n        \"\"\"\n        Runs all heuristic analyses on a single task and returns its profile.\n        \"\"\"\n        profiler.start(f\"TaskAnalyzer.analyze.{task_id}\")\n        \n        # 1. Get Difficulty Score (from Cell 2)\n        difficulty_score = estimate_task_difficulty(task)\n        if difficulty_score < 7.0:\n            tier = 'easy'\n        elif difficulty_score < 18.0:\n            tier = 'medium'\n        else:\n            tier = 'hard'\n\n        # 2. Detect Attractor Basin (Task Type)\n        basin = self._detect_basin(task.get('train', []))\n        \n        # 3. --- LTM-v4 STEP: Generate Delta-Fingerprint ---\n        try:\n            delta_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            delta_fingerprint = None \n \n        # 4. Create the \"Task Passport\"\n        profile = TaskProfile(\n            task_id=task_id,\n            difficulty_tier=tier,\n            basin=basin,\n            delta_fingerprint=delta_fingerprint,\n            difficulty_score=difficulty_score \n        )\n        \n        profiler.end(f\"TaskAnalyzer.analyze.{task_id}\")\n        return profile\n\n    def _detect_basin(self, train_examples: List[Dict]) -> str:\n        \"\"\"Heuristically determines the 'type' of task (a fast, weak heuristic).\"\"\"\n        if not train_examples:\n            return 'unknown'\n        \n        features = {'has_shape_change': False, 'has_color_change': False, \n                    'has_object_count_change': False}\n        \n        for example in train_examples:\n            try:\n                inp, out = np.array(example['input']), np.array(example['output'])\n                if inp.shape != out.shape: \n                    features['has_shape_change'] = True\n                if not np.array_equal(inp, out):\n                    features['has_color_change'] = True\n            except Exception:\n                continue\n        \n        if features['has_shape_change']: return 'scaling'\n        if features['has_color_change']: return 'color_mapping'\n        return 'unknown'\n\nprint(\"  Defined: TaskAnalyzer (The Triage Engine)\")\nprint(\"=\"*70)\n#Cell 5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:40:11.111982Z","iopub.execute_input":"2025-11-09T12:40:11.112315Z","iopub.status.idle":"2025-11-09T12:40:11.211140Z","shell.execute_reply.started":"2025-11-09T12:40:11.112293Z","shell.execute_reply":"2025-11-09T12:40:11.210027Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 5 (v2.0) Fingerprinting & Triage\n  Defined: TaskProfile (The AGI's 'Task Passport')\n  Defined: VisionModelEncoder (Grid-to-Vector Engine)\n  Defined: TaskFingerprinter (The 'Probe' Generator)\n  Defined: TaskAnalyzer (The Triage Engine)\n======================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#Cell 6\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 6: The Cognitive Engine (DSS/CWM Core)\n#\n# *** v2.0 REBUILD (DSS/CWM Core) ***\n#\n# 1. `SymbolicProgramInterpreter` (CPU):\n#    - Refactored to be state-based. `run_instruction` now takes a\n#      `CognitiveState` and returns a *new* `CognitiveState` as\n#      computed by the CWM primitives (Cell 4).\n#\n# 2. `SymbolicProgramSynthesizer` (Mind):\n#    - **`_precompute_finders` DELETED.** This function was the core\n#      bottleneck (3.3M calls) and is now obsolete.\n#    - `solve` is rebuilt. It now creates *one* initial `CognitiveState` by\n#      calling the `PerceptionEngine` *once*.\n#    - The beam search now expands nodes by calling CWM primitives, which\n#      perform *symbolic updates* to the state, eliminating all\n#      N+1 perception calls.\n#    - `_get_possible_ops` is now a lightweight, symbolic function that\n#      reads from the current state's `symbolic_state` and `context`.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(f\"üåä‚öõÔ∏è LucidOrca Solver: Cell 6 (v2.0) The Cognitive Engine (DSS/CWM)\")\n\n# --- 1. The \"CPU\" - Executes Programs on a CognitiveState ---\n\nclass SymbolicProgramInterpreter:\n    \"\"\"\n    The v2.0 \"CPU\" (Interpreter).\n    It executes an AST (program) by applying CWM primitives (Cell 4)\n    to a `CognitiveState` (Cell 2).\n    \"\"\"\n    \n    def __init__(self, perception_engine: PerceptionEngine):\n        # We need the perception engine for two reasons:\n        # 1. To pass to the CWM primitives (Cell 4)\n        # 2. To create the *initial* state for the Synthesizer\n        self.perception_engine = perception_engine\n        \n        # The \"ALU\" (Instruction Set) from Cell 4\n        self.primitives = MetaPrimitives(self.perception_engine)\n        \n        # This \"dispatch table\" maps 'op' strings from the AST\n        # to the actual Python functions in our CWM class.\n        self.dispatch_table: Dict[str, Callable] = {\n            # --- Selector Ops (Symbolic) ---\n            'find': self.primitives.find_objects,\n            'get_largest': self.primitives.get_largest,\n            'get_smallest': self.primitives.get_smallest,\n            \n            # --- Causal Transformer Ops (Symbolic + Pixel) ---\n            'move': self.primitives.move,\n            'delete': self.primitives.delete,\n            'recolor': self.primitives.recolor,\n            'copy': self.primitives.copy_objects,\n            'draw_path': self.primitives.draw_path,\n            \n            # --- Procedural Op ---\n            'map': self._op_map \n        }\n        print(f\"  SymbolicProgramInterpreter (v2.0 'CPU') initialized.\")\n\n    def run_instruction(self, state: CognitiveState, \n                        instruction: Dict) -> CognitiveState:\n        \"\"\"\n        Takes a state and an instruction, and returns the\n        *new state* produced by the CWM primitive.\n        \"\"\"\n        op_name = instruction.get('op')\n        params = instruction.get('params', {})\n        \n        if op_name not in self.dispatch_table:\n            print(f\"  ‚ö†Ô∏è  Interpreter Error: Unknown operation: {op_name}\")\n            return state # Return original state\n            \n        op_function = self.dispatch_table[op_name]\n        \n        try:\n            new_state = op_function(state, **params)\n            # The primitive returns a new state. We just add the instruction\n            # to its history.\n            new_state.program_ast = state.program_ast + [instruction]\n            return new_state\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Interpreter Error: Op '{op_name}' crashed: {e}\")\n            return state # Return original state\n            \n    def run(self, program_ast: List[Dict], initial_grid: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Executes a full program AST on a *raw grid*.\n        This is the public-facing \"run\" function used for validation\n        and final submission.\n        \"\"\"\n        profiler.start(\"v2.Interpreter.run\")\n        \n        # --- v2.0 DSS Step 1: \"Pixel-to-Symbol\" Bridge ---\n        # Perceive the *initial* grid *once* to create the root state.\n        try:\n            initial_symbols = self.perception_engine.analyze(initial_grid)\n            # The 'all_objects' key is a special context key used by _get_possible_ops\n            initial_context = {'all_objects': initial_symbols, 'last_result': initial_symbols}\n            \n            current_state = CognitiveState(\n                program_ast=[],\n                grid_state=initial_grid.copy(),\n                symbolic_state=initial_symbols,\n                context=initial_context,\n                cost=0.0\n            )\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Interpreter.run Error: Initial perception failed: {e}\")\n            profiler.end(\"v2.Interpreter.run\")\n            return initial_grid\n\n        # --- v2.0 DSS Step 2: \"Symbolic Execution\" Loop ---\n        # Run the program. Each instruction updates the state.\n        try:\n            for instruction in program_ast:\n                current_state = self.run_instruction(current_state, instruction)\n            \n            profiler.end(\"v2.Interpreter.run\")\n            # Return the final pixel-state \"territory\"\n            return current_state.grid_state\n        \n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Interpreter.run Error: Program execution failed: {e}\")\n            profiler.end(\"v2.Interpreter.run\")\n            return initial_grid # Fallback to original grid on crash\n\n    def _op_map(self, state: CognitiveState, **params) -> CognitiveState:\n        \"\"\"\n        Procedural Op: Applies a sub-program to each object in a list.\n        This is now *much* cleaner in v2.0.\n        \"\"\"\n        target_key = params.get('target', 'last_result')\n        sub_program = params.get('program', []) \n        \n        if not sub_program: return state \n        items_to_map = state.context.get(target_key, [])\n        if not items_to_map or not isinstance(items_to_map, list): return state \n        \n        map_results = []\n        current_state = state\n        \n        # Iterate over a *static list* of the object IDs to map\n        # This is crucial because the `current_state` (and its symbol map)\n        # will change with every iteration.\n        target_ids = [item.obj_id for item in items_to_map if hasattr(item, 'obj_id')]\n        \n        for obj_id in target_ids:\n            try:\n                # Find the *current version* of the object in the *current state*\n                target_obj = next((obj for obj in current_state.symbolic_state if obj.obj_id == obj_id), None)\n                if target_obj is None:\n                    continue # Object was deleted by a previous map iteration\n\n                # Create a local state for the sub-program\n                # 'this' points to the *single object* we are mapping over\n                local_context = {'this': [target_obj], 'last_result': [target_obj]}\n                \n                # We \"fork\" the state for this sub-program\n                local_state = CognitiveState(\n                    program_ast=[], # Sub-program has its own history\n                    grid_state=current_state.grid_state,\n                    symbolic_state=current_state.symbolic_state,\n                    context=local_context,\n                    cost=0.0\n                )\n                \n                # Run the sub-program (e.g., [{'op': 'recolor', 'params': ...}])\n                for instruction in sub_program:\n                    local_state = self.run_instruction(local_state, instruction)\n                \n                # The \"main\" state becomes the result of the last sub-program run\n                current_state = local_state\n                \n                # Store the sub-program's result\n                map_results.append(current_state.context.get('last_result'))\n\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è  Interpreter._op_map Error: Sub-program failed: {e}\")\n                continue\n        \n        # After the loop, update the main state's context\n        current_state.context['last_result'] = map_results\n        return current_state\n\nprint(f\"  Defined: SymbolicProgramInterpreter (v2.0 'CPU')\")\n\n\n# --- 2. The \"Mind\" - Generates Programs via Symbolic Search ---\n\nclass SymbolicProgramSynthesizer:\n    \"\"\"\n    The v2.0 \"Mind\" (Synthesizer).\n    \n    This class is now a true symbolic-space searcher. It operates on\n    `CognitiveState` objects, using the CWM primitives to expand its\n    search tree.\n    \n    **The `_precompute_finders` bottleneck is GONE.**\n    \"\"\"\n    \n    # --- *** (HOTFIX 19) *** ---\n    TOP_K_ACTIONS: int = 15 # Hyper-pruning\n    \n    def __init__(self, interpreter: SymbolicProgramInterpreter, \n                 cfg: ChampionshipConfig, \n                 fingerprinter: TaskFingerprinter,\n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List,\n                 hpn_grammar: Dict):\n        \n        self.interpreter = interpreter\n        # --- *** v2.0: We need the perception engine *directly* ---\n        self.perception_engine = interpreter.perception_engine\n        \n        self.config = cfg\n        \n        # --- HPN / LTM Components ---\n        self.fingerprinter = fingerprinter\n        self.hpn_playbook_vectors = hpn_playbook_vectors\n        self.hpn_playbook_programs = hpn_playbook_programs\n        self.hpn_grammar = hpn_grammar\n        \n        if self.hpn_playbook_vectors is not None and self.hpn_grammar:\n            print(f\"  SymbolicProgramSynthesizer (v2.0 'Schooled Mind') initialized. [HPN LOADED]\")\n        else:\n            print(f\"  SymbolicProgramSynthesizer (v2.0 'Schooled Mind') initialized. [HPN *NOT* LOADED]\")\n\n    def _heuristic(self, grid: np.ndarray, target_grid: np.ndarray) -> float:\n        \"\"\"The \"cost\" function (pixel-distance).\"\"\"\n        if grid.shape != target_grid.shape:\n            return 1000.0 + abs(grid.size - target_grid.size)\n        return np.sum(grid != target_grid)\n\n    def _get_possible_ops(self, state: CognitiveState) -> List[Dict]:\n        \"\"\"\n        v2.0: This is now a lightweight, symbolic-state-based action generator.\n        It *reads* from `state.symbolic_state` and `state.context`.\n        It **DOES NOT** call the perception engine.\n        \"\"\"\n        profiler.start(\"v2.Synthesizer._get_possible_ops\")\n        \n        possible_ops = []\n        \n        # Find all valid \"targets\" (lists of objects) in our current context\n        target_keys = [\n            k for k, v in state.context.items() \n            if isinstance(v, list) and k != 'last_result' and v\n        ]\n        \n        if not target_keys:\n            # If no context, create a \"finder\" op for every color\n            # on the *symbolic map* to bootstrap the search.\n            all_colors = {obj.color for obj in state.symbolic_state if obj.color != 0}\n            for c in all_colors:\n                possible_ops.append({'op': 'find', 'params': {'color': c}})\n            \n            # Add \"find all\" as a bootstrap\n            possible_ops.append({'op': 'find', 'params': {}})\n            \n            profiler.end(\"v2.Synthesizer._get_possible_ops\")\n            return possible_ops\n            \n        # --- Generate \"Transformer\" and \"Procedural\" ops ---\n        deltas = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n        \n        for target_key in target_keys:\n            # 1. Simple \"Global\" Transformer Ops\n            for color in range(10):\n                possible_ops.append(\n                    {'op': 'recolor', 'params': {'target': target_key, 'color': color}}\n                )\n            \n            for d in deltas:\n                possible_ops.append({'op': 'move', 'params': {'target': target_key, 'delta': d}})\n                possible_ops.append({'op': 'copy', 'params': {'target': target_key, 'delta': d}})\n                \n            possible_ops.append({'op': 'delete', 'params': {'target': target_key}})\n            \n            # 2. Drawing Primitives\n            for color in range(1, 10): # Iterate 1-9\n                possible_ops.append(\n                    {'op': 'draw_path', 'params': {'target': target_key, 'color': color}}\n                )\n        \n            # 3. Procedural Ops (ForEach loop)\n            map_sub_programs = []\n            map_target = 'this' # The keyword for the item being mapped\n            \n            for color in range(10):\n                map_sub_programs.append(\n                    [{'op': 'recolor', 'params': {'target': map_target, 'color': color}}]\n                )\n            map_sub_programs.append([{'op': 'delete', 'params': {'target': map_target}}])\n            \n            for d in deltas:\n                map_sub_programs.append([{'op': 'move', 'params': {'target': map_target, 'delta': d}}])\n                map_sub_programs.append([{'op': 'copy', 'params': {'target': map_target, 'delta': d}}])\n            \n            for sub_prog in map_sub_programs:\n                possible_ops.append({\n                    'op': 'map',\n                    'params': {'target': target_key, 'program': sub_prog}\n                })\n        \n        # --- Generate \"Selector\" ops to refine context ---\n        # These ops are \"free\" (cost 0) in the search, as they only filter\n        # the symbolic state.\n        possible_ops.append({'op': 'get_largest', 'params': {'target': 'last_result'}})\n        possible_ops.append({'op': 'get_smallest', 'params': {'target': 'last_result'}})\n        all_colors = {obj.color for obj in state.symbolic_state if obj.color != 0}\n        for c in all_colors:\n            possible_ops.append({'op': 'find', 'params': {'color': c}})\n\n        profiler.end(\"v2.Synthesizer._get_possible_ops\")\n        return possible_ops\n\n    def _get_playbook_seeds(self, task: Dict, \n                          initial_state: CognitiveState) -> List[CognitiveState]:\n        \"\"\"\n        v2.0: Phase A (\"Diagnose & Seed\")\n        Runs HPN playbook programs and returns a list of *new CognitiveStates*\n        to \"seed\" the beam search.\n        \"\"\"\n        profiler.start(\"v2.Synthesizer.get_playbook_seeds\")\n        seed_states = []\n        target_grid = np.array(task['train'][0]['output'])\n        \n        # 1. Diagnose: Fingerprint the new task\n        try:\n            task_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            profiler.end(\"v2.Synthesizer.get_playbook_seeds\")\n            return [] \n\n        # 2. k-NN Lookup: Find closest \"quiz\" tasks\n        if (self.hpn_playbook_vectors is None or \n            len(self.hpn_playbook_programs) == 0):\n            profiler.end(\"v2.Synthesizer.get_playbook_seeds\")\n            return []\n\n        try:\n            distances = np.linalg.norm(self.hpn_playbook_vectors - task_fingerprint, axis=1)\n            k = 3 \n            nearest_indices = np.argsort(distances)[:k]\n        except Exception:\n            profiler.end(\"v2.Synthesizer.get_playbook_seeds\")\n            return []\n\n        # 3. Load Architectures: Run seed programs on the *initial state*\n        for i in nearest_indices:\n            seed_program = self.hpn_playbook_programs[i]\n            \n            try:\n                # Start from the *initial state*\n                current_seed_state = initial_state \n                \n                for instruction in seed_program:\n                    # Run the program, updating the state symbolically\n                    current_seed_state = self.interpreter.run_instruction(\n                        current_seed_state, instruction\n                    )\n                \n                # Calculate cost of the final state\n                seed_cost = self._heuristic(current_seed_state.grid_state, target_grid)\n                current_seed_state.cost = seed_cost\n                \n                seed_states.append(current_seed_state)\n            except Exception:\n                continue\n\n        profiler.end(\"v2.Synthesizer.get_playbook_seeds\")\n        return seed_states\n\n    def solve(self, task: Dict, timeout: float) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        v2.0: This is the refactored DSS/CWM search loop.\n        It calls perception *once* and then searches symbolically.\n        \"\"\"\n        profiler.start(\"v2.SymbolicProgramSynthesizer.solve\")\n        task_start_time = time.time()\n        \n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n            return None, \"Synthesizer.NoTrainData\"\n\n        try:\n            initial_grid = np.array(examples[0]['input'])\n            target_grid = np.array(examples[0]['output'])\n        except Exception:\n            profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n            return None, \"Synthesizer.BadData\"\n            \n        initial_cost = self._heuristic(initial_grid, target_grid)\n        if initial_cost == 0:\n            return [], \"Synthesizer.Identity\"\n        \n        # --- v2.0 STEP 1: PERCEIVE ONCE ---\n        try:\n            initial_symbols = self.perception_engine.analyze(initial_grid)\n            initial_context = {'all_objects': initial_symbols, 'last_result': initial_symbols}\n            \n            initial_state = CognitiveState(\n                program_ast=[],\n                grid_state=initial_grid,\n                symbolic_state=initial_symbols,\n                context=initial_context,\n                cost=initial_cost\n            )\n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Synthesizer.solve Error: Initial perception failed: {e}\")\n            profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n            return None, \"Synthesizer.Fail.Perception\"\n        \n        \n        # --- Phase A (\"Playbook Seeding\") ---\n        beam = self._get_playbook_seeds(task, initial_state)\n        \n        # Add the \"empty program\" state to the beam\n        beam.append(initial_state)\n        \n        # Visited set now checks hashes of CognitiveState (grid.tobytes())\n        visited_states: Dict[bytes, float] = {\n            entry.grid_state.tobytes(): entry.cost for entry in beam\n        }\n\n        # --- Phase B (\"Grammar-Guided & Pruned Symbolic Search\") ---\n        \n        for depth in range(self.config.MAX_PROGRAM_DEPTH):\n            if (time.time() - task_start_time) > timeout:\n                profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n                return None, \"Synthesizer.Timeout.Coarse\"\n                \n            new_beam = []\n            \n            for entry_idx, current_state in enumerate(beam):\n                \n                if (entry_idx % (max(1, self.config.BEAM_SEARCH_WIDTH // 10)) == 0):\n                     if (time.time() - task_start_time) > timeout:\n                        profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n                        return None, \"Synthesizer.Timeout.Granular\"\n                \n                # --- v2.0: Get lightweight symbolic actions ---\n                possible_ops = self._get_possible_ops(current_state)\n                \n                # --- HPN Guidance (Level 2: \"Grammar\") ---\n                if not current_state.program_ast:\n                    last_op_name = \"START\"\n                else:\n                    last_op_name = current_state.program_ast[-1].get('op', 'unknown')\n                \n                hpn_probabilities = self.hpn_grammar.get(last_op_name, {})\n                \n                prioritized_ops = sorted(\n                    possible_ops,\n                    key=lambda op: hpn_probabilities.get(op.get('op'), 0.0),\n                    reverse=True\n                )\n                \n                # --- HOTFIX 19: HYPER-PRUNING ---\n                ops_to_evaluate = prioritized_ops[:self.TOP_K_ACTIONS]\n                \n                for op_ast in ops_to_evaluate:\n                    try:\n                        # --- v2.0: THIS IS THE CORE CWM LOOP ---\n                        # This call runs the CWM primitive, which returns\n                        # a *new state* with updated pixels AND symbols.\n                        # ** NO RE-PERCEPTION IS REQUIRED. **\n                        new_state = self.interpreter.run_instruction(\n                            current_state, \n                            op_ast\n                        )\n                        # --- End CWM Call ---\n                        \n                        new_grid_hash = new_state.grid_state.tobytes()\n                        new_cost = self._heuristic(new_state.grid_state, target_grid)\n                        new_state.cost = new_cost\n                        \n                        if visited_states.get(new_grid_hash, float('inf')) <= new_cost:\n                            continue\n                        visited_states[new_grid_hash] = new_cost\n\n                        if new_cost == 0.0:\n                            # We have a match on example 0. Validate on all.\n                            if self._validate_program(new_state.program_ast, examples):\n                                profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n                                return new_state.program_ast, f\"Synthesizer.Success.d{len(new_state.program_ast)}\"\n                        \n                        new_beam.append(new_state)\n                    \n                    except Exception as e:\n                        # This branch should rarely be hit due to try/except\n                        # inside run_instruction, but is a good safeguard.\n                        print(f\"  ‚ö†Ô∏è  Synthesizer.solve Error: Op expand failed: {e}\")\n                        continue \n            \n            if not new_beam:\n                break \n            \n            # Combine, sort, and prune the beam\n            new_beam.sort() # Use the __lt__ method on CognitiveState\n            \n            seen_grids = set()\n            unique_beam = []\n            for entry in new_beam:\n                grid_hash = entry.grid_state.tobytes()\n                if grid_hash not in seen_grids:\n                    unique_beam.append(entry)\n                    seen_grids.add(grid_hash)\n            \n            beam = unique_beam[:self.config.BEAM_SEARCH_WIDTH]\n\n        profiler.end(\"v2.SymbolicProgramSynthesizer.solve\")\n        return None, \"Synthesizer.Fail.MaxDepth\"\n\n    def _validate_program(self, program_ast: List[Dict], examples: List[Dict]) -> bool:\n        \"\"\"\n        Validates a candidate program against ALL training examples.\n        Uses the v2.0 interpreter's public `run` method.\n        \"\"\"\n        for ex in examples[1:]: # Start from example 1 (0 was solved in search)\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                \n                # interpreter.run handles the full v2.0 \"perceive-once\"\n                # and \"symbolic-run\" loop internally.\n                predicted_grid = self.interpreter.run(program_ast, inp_grid)\n                \n                if not np.array_equal(predicted_grid, out_grid):\n                    return False\n            except Exception:\n                return False \n                \n        return True\n\nprint(f\"  Defined: SymbolicProgramSynthesizer (v2.0 'Schooled Mind')\")\nprint(\"=\"*70)\n#Cell 6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:40:31.034774Z","iopub.execute_input":"2025-11-09T12:40:31.035126Z","iopub.status.idle":"2025-11-09T12:40:31.084118Z","shell.execute_reply.started":"2025-11-09T12:40:31.035101Z","shell.execute_reply":"2025-11-09T12:40:31.083051Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 6 (v2.0) The Cognitive Engine (DSS/CWM)\n  Defined: SymbolicProgramInterpreter (v2.0 'CPU')\n  Defined: SymbolicProgramSynthesizer (v2.0 'Schooled Mind')\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Cell 7\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 7: The Master Toolbox (Orchestrator & Assembly)\n#\n# *** v2.0 REBUILD (DSS/CWM Integration) ***\n#\n# 1. `HeuristicPlaybookSolver` (\"Fast Brain\"):\n#    - Its `_validate_program` method is refactored to use the new\n#      `v2.0 interpreter.run()` method. This ensures both \"brains\"\n#      use the same state-aware execution engine for validation.\n#\n# 2. `LucidOrcaUltimateSolver` (\"Toolbox\"):\n#    - The `__init__` method is now a v2.0 \"assembly line.\" It\n#      instantiates the refactored `PerceptionEngine` (Cell 3),\n#      the refactored CWM `MetaPrimitives` (Cell 4), and the\n#      refactored `SymbolicProgramInterpreter/Synthesizer` (Cell 6),\n#      wiring them all together correctly.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(f\"üåä‚öõÔ∏è LucidOrca Solver: Cell 7 (v2.0) Master Toolbox & Assembly\")\n\n# --- 1. Consensus Utility (Unchanged) ---\n\nclass SolverAgreementEnsemble:\n    \"\"\"\n    Measures solver agreement. This is a utility for our final step\n    to \"judge\" the *results* (grids) from the Abstraction (LTM-v4 Query)\n    and Reasoning (LTM-v4 Search) passes.\n    \"\"\"\n    def __init__(self):\n        self.agreement_history = []\n        print(\"  SolverAgreementEnsemble (Consensus Utility) initialized.\")\n\n    def measure_agreement(self, grids: List[np.ndarray]) -> Tuple[float, np.ndarray]:\n        if not grids:\n            return 0.0, np.array([[0]])\n\n        grid_hashes = []\n        valid_grids = []\n        \n        for grid in grids:\n            if grid is not None and grid.size > 0:\n                grid_hashes.append(grid.tobytes())\n                valid_grids.append(grid)\n            else:\n                grid_hashes.append(\"NONE\")\n        \n        if not valid_grids:\n             return 0.0, np.array([[0]])\n\n        counts = Counter(grid_hashes)\n        most_common_bytes, most_common_count = counts.most_common(1)[0]\n        \n        agreement_ratio = most_common_count / len(grid_hashes)\n\n        if most_common_bytes != \"NONE\":\n            collapsed_solution = next(g for g in valid_grids if g.tobytes() == most_common_bytes)\n        else:\n            if len(valid_grids) > 0:\n                collapsed_solution = valid_grids[0]\n            else:\n                collapsed_solution = np.array([[0]])\n\n        self.agreement_history.append(agreement_ratio)\n        return agreement_ratio, collapsed_solution\n\nprint(\"  Defined: SolverAgreementEnsemble (Consensus Utility)\")\n\n\n# --- \"Brain 1\" (The \"Fast\" Architect - v2.0 Compliant) ---\n\nclass HeuristicPlaybookSolver:\n    \"\"\"\n    This is our \"fast\" brain (\"Brain 1\"). It is the \"schooled\"\n    architect. It does *not* do a beam search. It only\n    checks its \"playbook\" of known-good architectures.\n    \n    v2.0: Now uses the new DSS/CWM interpreter for validation.\n    \"\"\"\n    def __init__(self, fingerprinter: TaskFingerprinter, \n                 interpreter: SymbolicProgramInterpreter, # <-- v2.0 Interpreter\n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List):\n        \n        self.fingerprinter = fingerprinter\n        self.interpreter = interpreter # The v2.0 (state-aware) interpreter\n        self.hpn_playbook_vectors = hpn_playbook_vectors\n        self.hpn_playbook_programs = hpn_playbook_programs\n        \n        if self.hpn_playbook_vectors is not None:\n            print(f\"  HeuristicPlaybookSolver (v2.0 'Fast Brain') initialized. [HPN LOADED]\")\n        else:\n            print(f\"  HeuristicPlaybookSolver (v2.0 'Fast Brain') initialized. [HPN *NOT* LOADED]\")\n\n    def _validate_program(self, program_ast: List[Dict], examples: List[Dict]) -> bool:\n        \"\"\"\n        *** v2.0 REFACTOR ***\n        Validates a candidate program against ALL training examples\n        using the new v2.0 interpreter's public `run` method.\n        \"\"\"\n        for ex in examples:\n            try:\n                inp_grid = np.array(ex['input'])\n                out_grid = np.array(ex['output'])\n                \n                # Use the v2.0 interpreter's public `run` method.\n                # This method correctly handles the \"perceive-once\"\n                # and \"symbolic-run\" process.\n                predicted_grid = self.interpreter.run(program_ast, inp_grid)\n                \n                if not np.array_equal(predicted_grid, out_grid):\n                    return False\n            except Exception:\n                return False \n        return True\n\n    def solve(self, task: Dict, timeout: float) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        This is the \"fast-pass\" cognitive loop:\n        1. Diagnose (fingerprint)\n        2. k-NN Lookup (check HPN Playbook)\n        3. Validate (run the 5 best-guess architectures using v2.0 interpreter)\n        \"\"\"\n        profiler.start(\"v2.HeuristicPlaybookSolver.solve\")\n        \n        examples = task.get('train', [])\n        if not examples:\n            profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.NoTrainData\"\n            \n        # 1. Diagnose: Fingerprint the new task\n        try:\n            task_fingerprint = self.fingerprinter.fingerprint(task)\n        except Exception:\n            profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.Fingerprint\"\n\n        # 2. k-NN Lookup: Find 5 closest \"quiz\" architectures\n        if (self.hpn_playbook_vectors is None or \n            len(self.hpn_playbook_programs) == 0):\n            profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.NoHPN\"\n\n        try:\n            distances = np.linalg.norm(self.hpn_playbook_vectors - task_fingerprint, axis=1)\n            k = 5 # Check the top 5 \"plays\"\n            nearest_indices = np.argsort(distances)[:k]\n        except Exception:\n            profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n            return None, \"Playbook.Fail.KNN\"\n\n        # 3. Validate: Run the \"Playbook\"\n        for i in nearest_indices:\n            seed_program = self.hpn_playbook_programs[i]\n            \n            # This now calls the refactored _validate_program\n            if self._validate_program(seed_program, examples):\n                profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n                return seed_program, \"Playbook.Success\"\n        \n        # If no \"play\" worked...\n        profiler.end(\"v2.HeuristicPlaybookSolver.solve\")\n        return None, \"Playbook.Fail.NoMatch\"\n\nprint(f\"  Defined: HeuristicPlaybookSolver (v2.0 'Fast Brain')\")\n\n\n# --- 2. The LTM-v4 Master \"Toolbox\" Class (v2.0 Assembly) ---\n\nclass LucidOrcaUltimateSolver:\n    \"\"\"\n    The main \"Factory Manager\" or \"Toolbox\" class for the v2.0 build.\n    This class *assembles* all the refactored components from Cells 3-6.\n    \"\"\"\n    def __init__(self, cfg: ChampionshipConfig, \n                 hpn_playbook_vectors: Optional[np.ndarray],\n                 hpn_playbook_programs: List,\n                 hpn_grammar: Dict):\n        \n        print(\"\\n\" + \"=\"*70)\n        print(f\"üåä‚öõÔ∏è Initializing LucidOrcaUltimateSolver (v2.0 Master Toolbox)...\")\n        \n        self.config = cfg\n        \n        # --- 1. Instantiate Core \"Vision\" (from Cell 3) ---\n        self.perception_engine = PerceptionEngine()\n        print(f\"  ‚úÖ 1. PerceptionEngine (Vision)... LOADED\")\n\n        # --- 2. Instantiate \"Triage & Fingerprinting\" (from Cell 5) ---\n        self.analyzer = TaskAnalyzer(self.perception_engine)\n        self.fingerprinter = self.analyzer.fingerprinter\n        print(f\"  ‚úÖ 2. TaskAnalyzer (Triage & Fingerprinting)... LOADED\")\n\n        # --- 3. Instantiate The v2.0 \"CPU\" (from Cell 6) ---\n        # This interpreter now understands CognitiveState\n        self.interpreter = SymbolicProgramInterpreter(self.perception_engine)\n        print(f\"  ‚úÖ 3. SymbolicProgramInterpreter (v2.0 'CPU')... LOADED\")\n\n        # --- 4. Instantiate The v2.0 \"Fast Brain\" (Brain 1) ---\n        # We pass it the v2.0 interpreter\n        self.heuristic_solver = HeuristicPlaybookSolver(\n            self.fingerprinter,\n            self.interpreter,\n            hpn_playbook_vectors,\n            hpn_playbook_programs\n        )\n        print(f\"  ‚úÖ 4. HeuristicPlaybookSolver (v2.0 'Fast Brain')... LOADED\")\n\n        # --- 5. Instantiate The v2.0 \"Schooled Slow Brain\" (Brain 2) ---\n        # We pass it the v2.0 interpreter and HPN components\n        self.synthesizer = SymbolicProgramSynthesizer(\n            self.interpreter, \n            self.config, \n            self.fingerprinter,\n            hpn_playbook_vectors,\n            hpn_playbook_programs,\n            hpn_grammar\n        )\n        print(f\"  ‚úÖ 5. SymbolicProgramSynthesizer (v2.0 'Schooled Mind')... LOADED\")\n        \n        # --- 6. Instantiate Utilities ---\n        self.utilities = {\n            'agreement_ensemble': SolverAgreementEnsemble()\n        }\n        print(\"  ‚úÖ 6. Consensus & Utility Modules... LOADED\")\n\n        print(\"\\n‚úÖ LucidOrca v2.0 Solver (Toolbox) is ready!\"); print(\"=\"*70)\n\nprint(\"=\"*70)\n#Cell 7\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:40:56.016965Z","iopub.execute_input":"2025-11-09T12:40:56.017335Z","iopub.status.idle":"2025-11-09T12:40:56.041305Z","shell.execute_reply.started":"2025-11-09T12:40:56.017310Z","shell.execute_reply":"2025-11-09T12:40:56.039757Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 7 (v2.0) Master Toolbox & Assembly\n  Defined: SolverAgreementEnsemble (Consensus Utility)\n  Defined: HeuristicPlaybookSolver (v2.0 'Fast Brain')\n======================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#Cell 8\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - (LTM-v4 REBUILD)\n#\n# Cell 8: Load All Task Data\n#\n# This cell defines the data loading functions and loads all necessary\n# JSON files from the Kaggle environment.\n#\n# 1. `test_tasks`: The set of tasks we must solve for submission.\n# 2. `training_tasks` & `evaluation_tasks`: The 1120 known tasks\n#    that our \"Game Genie\" (Cell 9) will use to \"train\" (build its LTM).\n# 3. `training_solutions` & `evaluation_solutions`: The ground-truth\n#    solutions required by Cell 9 to *validate* the rules it finds.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(\"üåä‚öõÔ∏è LucidOrca Solver: Cell 8 (LTM-v4) Load Task Data\")\n\n# --- 1. Define Data Paths ---\\n# This assumes the standard Kaggle competition dataset path\nDATA_DIR = Path(\"/kaggle/input/arc-prize-2025\")\n\n# The \"unknown\" tasks for our final submission\nTEST_CHALLENGES_PATH = DATA_DIR / \"arc-agi_test_challenges.json\"\n\n# The \"known\" tasks for our Game Genie (LTM)\nTRAINING_CHALLENGES_PATH = DATA_DIR / \"arc-agi_training_challenges.json\"\nEVALUATION_CHALLENGES_PATH = DATA_DIR / \"arc-agi_evaluation_challenges.json\"\n\n# The \"ground truth\" for our Game Genie (LTM)\nTRAINING_SOLUTIONS_PATH = DATA_DIR / \"arc-agi_training_solutions.json\"\nEVALUATION_SOLUTIONS_PATH = DATA_DIR / \"arc-agi_evaluation_solutions.json\"\n\n\n# --- 2. Data Loading Function ---\\n\ndef load_json_tasks(file_path: Path) -> Dict[str, Dict]:\n    \"\"\"\n    Loads a JSON task file from the given path.\n    Returns a dictionary of {task_id: task_data}.\n    \"\"\"\n    if not file_path.exists():\n        print(f\"  ‚ö†Ô∏è  WARNING: Task file not found at: {file_path}\")\n        print(f\"     Please ensure the 'ARC Prize 2025' dataset is added to this notebook.\")\n        return {}\n    \n    try:\n        profiler.start(f\"load_json.{file_path.name}\")\n        with open(file_path, 'r') as f:\n            tasks = json.load(f)\n        profiler.end(f\"load_json.{file_path.name}\")\n        \n        print(f\"  ‚úÖ Loaded {len(tasks)} tasks from {file_path.name}\")\n        return tasks\n        \n    except Exception as e:\n        print(f\"  ‚ùå CRITICAL ERROR: Could not load or parse {file_path.name}: {e}\")\n        return {}\n\n# --- 3. Load the Test Set (Primary Goal) ---\\nprint(\"\\nLoading test set for submission...\")\ntest_tasks = load_json_tasks(TEST_CHALLENGES_PATH)\n\nif test_tasks:\n    print(f\"\\nSample task IDs from test set:\")\n    for i, task_id in enumerate(list(test_tasks.keys())[:3]):\n        try:\n            task = test_tasks[task_id]\n            print(f\"   {i+1}. {task_id}: {len(task.get('test', []))} test cases\")\n        except Exception as e:\n            print(f\"   {i+1}. {task_id}: Error parsing task info - {e}\")\nelse:\n    print(\"  No test tasks loaded. Submission will not be possible.\")\n\n# --- 4. Load Training/Evaluation Data (for \"Game Genie\" in Cell 9) ---\nprint(\"\\nLoading supplemental data for 'Game Genie' LTM pre-computation...\")\ntraining_tasks = load_json_tasks(TRAINING_CHALLENGES_PATH)\ntraining_solutions = load_json_tasks(TRAINING_SOLUTIONS_PATH)\nevaluation_tasks = load_json_tasks(EVALUATION_CHALLENGES_PATH)\nevaluation_solutions = load_json_tasks(EVALUATION_SOLUTIONS_PATH)\n\nprint(\"=\"*70)\n#Cell 8\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:41:06.557995Z","iopub.execute_input":"2025-11-09T12:41:06.558386Z","iopub.status.idle":"2025-11-09T12:41:07.106656Z","shell.execute_reply.started":"2025-11-09T12:41:06.558359Z","shell.execute_reply":"2025-11-09T12:41:07.105660Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 8 (LTM-v4) Load Task Data\n  ‚úÖ Loaded 240 tasks from arc-agi_test_challenges.json\n\nSample task IDs from test set:\n   1. 00576224: 1 test cases\n   2. 007bbfb7: 1 test cases\n   3. 009d5c81: 1 test cases\n\nLoading supplemental data for 'Game Genie' LTM pre-computation...\n  ‚úÖ Loaded 1000 tasks from arc-agi_training_challenges.json\n  ‚úÖ Loaded 1000 tasks from arc-agi_training_solutions.json\n  ‚úÖ Loaded 120 tasks from arc-agi_evaluation_challenges.json\n  ‚úÖ Loaded 120 tasks from arc-agi_evaluation_solutions.json\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Cell 9\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 9: RSC Controller & \"Game Genie\" LTM Trainer (v2.0)\n#\n# *** v2.0 REBUILD (DSS/CWM Integration) ***\n#\n# 1. This cell's logic remains largely the same, but it now *instantiates*\n#    and *uses* the v2.0-refactored components from Cells 4, 6, and 7.\n# 2. `__init__`: When `LucidOrcaUltimateSolver` is called, it now\n#    builds the full v2.0 DSS/CWM stack automatically.\n# 3. `_find_ground_truth_program`: The call to `slow_solver.solve()`\n#    now executes our new, fast symbolic search from Cell 6. This\n#    is the critical fix that will allow the LTM training to\n#    *finally* find and cache programs.\n# 4. HGI (SOTA Feature 4) is *planned* for `_run_architectural_bootcamp`\n#    but is deferred to v2.1. We must first *cache* programs before\n#    we can *induce* a grammar from them.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(f\"üåä‚öõÔ∏è LucidOrca Solver: Cell 9 (v2.0) RSC Controller & LTM Trainer\")\n\n# --- 1. Define the RSC Controller (LTM-v4) ---\n\nclass RSC_Controller:\n    \"\"\"\n    Implements the Recursive Symbolic Coherence (RSC) Framework.\n    v2.0: Manages the \"Two-Brain\" HPN architecture and runs the\n    \"Game Genie\" LTM training using the new DSS/CWM synthesizer.\n    \"\"\"\n    \n    # --- LTM Training Budget (HOTFIX 16) ---\n    LTM_BUDGET_PERCENT: float = CONFIG.LTM_BUDGET_PERCENT\n    LTM_NUM_TASKS: float = 1120.0 # 1000 train + 120 eval\n    \n    # --- Log Headers (HOTFIX 9) ---\n    LTM_LOG_COLUMNS = [\n        \"phase\", \"task_id\", \"task_tier\", \"status\", \n        \"time_taken_s\", \"program_cached\"\n    ]\n    INFERENCE_LOG_COLUMNS = [\n        \"phase\", \"task_id\", \"task_tier\", \"basin\", \n        \"ltm_status\", \"reasoning_status\", \"final_program\", \"time_taken_s\"\n    ]\n    \n    def __init__(self, cfg: ChampionshipConfig):\n        print(\"\\nInitializing RSC_Controller (v2.0 Architecture)...\")\n        self.config = cfg\n        \n        self.log_path = Path(\"/kaggle/working/lucid_metrics.csv\")\n        self.metric_logger = MetricLogger(self.log_path)\n        \n        # --- v2.0: This logic is now correct ---\n        # 1. Build a *temporary* \"Toolbox\" just to get the fingerprinter\n        print(\"  Bootstrapping Cognitive Architecture (Phase 1)...\")\n        # Note: This calls the *original* Cell 7 `__init__`\n        bootcamp_toolbox = LucidOrcaUltimateSolver(cfg, None, [], {}) \n        \n        # 2. Run the bootcamp to build the HPN \"Playbook\" (HGI v2.1 deferred)\n        print(\"  Bootstrapping Cognitive Architecture (Phase 2)...\")\n        profiler.start(\"GameGenie.Bootcamp\")\n        (\n            self.hpn_playbook_vectors,\n            self.hpn_playbook_programs,\n            self.hpn_grammar\n        ) = self._run_architectural_bootcamp(bootcamp_toolbox.fingerprinter)\n        profiler.end(\"GameGenie.Bootcamp\")\n        print(f\"  ‚úÖ Cognitive Bootcamp complete. Built HPN Grammar ({len(self.hpn_grammar)} rules) \"\n              f\"and HPN Playbook ({len(self.hpn_playbook_programs)} seeds).\")\n        \n        # 3. Build the *real* \"Two-Brain\" Toolbox (now v2.0-compliant)\n        # This one instantiates our refactored Cells 4 & 6.\n        self.solver_toolbox = LucidOrcaUltimateSolver(\n            cfg,\n            self.hpn_playbook_vectors,\n            self.hpn_playbook_programs,\n            self.hpn_grammar\n        )\n        \n        # --- LTM-v4: \"Game Genie\" Long-Term Memory ---\n        self._ltm_vectors_list: List[np.ndarray] = []\n        self.ltm_programs: List[List[Dict]] = [] \n        self.ltm_vectors: Optional[np.ndarray] = None \n        self.task_profiles: Dict[str, TaskProfile] = {}\n        self.time_allocations = {\n            'abstraction_per_task': {},\n            'reasoning_per_task': {},\n        }\n        print(\"  ‚úÖ RSC_Controller is online (v2.0 DSS/CWM Architecture).\")\n\n    def _run_architectural_bootcamp(self, fingerprinter: TaskFingerprinter\n                                   ) -> Tuple[Optional[np.ndarray], List, Dict]:\n        \"\"\"\n        Builds both HPNs (Level 1 Playbook, Level 2 Grammar).\n        \n        NOTE for v2.1: This function is the target for SOTA Feature 4 (HGI).\n        After `run_pre_computation` runs, we will re-run this function,\n        passing in `self.ltm_programs` and replacing the static `CURRICULUM`\n        with a data-driven grammar induced from *actual solved programs*.\n        \"\"\"\n        \n        # 1. The \"Curriculum\" (Core Architectures)\n        CURRICULUM = [\n            # v2.0: These ASTs are still valid for the new interpreter\n            [{'op': 'find'}, {'op': 'recolor'}], [{'op': 'find'}, {'op': 'move'}],\n            [{'op': 'find'}, {'op': 'copy'}], [{'op': 'find'}, {'op': 'delete'}],\n            [{'op': 'get_largest'}, {'op': 'recolor'}], [{'op': 'get_smallest'}, {'op': 'move'}],\n            [{'op': 'get_largest'}, {'op': 'copy'}], [{'op': 'get_smallest'}, {'op': 'delete'}],\n            [{'op': 'find'}, {'op': 'draw_path'}], [{'op': 'get_largest'}, {'op': 'draw_path'}],\n            [{'op': 'find'}, {'op': 'map', 'params': {'program': [{'op': 'recolor'}]}}],\n            [{'op': 'find'}, {'op': 'map', 'params': {'program': [{'op': 'move'}]}}],\n            [{'op': 'find'}, {'op': 'map', 'params': {'program': [{'op': 'delete'}]}}],\n            [{'op': 'find', 'params': {}}, {'op': 'copy'}, {'op': 'find'}, {'op': 'recolor'}],\n            [{'op': 'find', 'params': {}}, {'op': 'move'}, {'op': 'find'}, {'op': 'delete'}],\n            [{'op': 'find', 'params': {}}, {'op': 'draw_path'}, {'op': 'find'}, {'op': 'recolor'}],\n            [{'op': 'find', 'params': {}}, {'op': 'map'}, {'op': 'find'}, {'op': 'move'}], \n            [{'op': 'find'}]\n        ]\n\n        # 2. The \"Pop Quiz\" (Atomic Tasks for Playbook)\n        QUIZ_TASKS = {\n            'recolor_all': {'task': {'train': [{'input': [[1]], 'output': [[2]]}]}, \n                            'arch': [{'op': 'find', 'params': {}}, \n                                     {'op': 'recolor', 'params': {'target': 'all_objects', 'color': 2}}]},\n            'move_all': {'task': {'train': [{'input': [[1,0]], 'output': [[0,1]]}]},\\\n                         'arch': [{'op': 'find', 'params': {}}, \n                                  {'op': 'move', 'params': {'target': 'all_objects', 'delta': (0,1)}}]},\n            'copy_all': {'task': {'train': [{'input': [[1,0]], 'output': [[1,1]]}]},\\\n                         'arch': [{'op': 'find', 'params': {}}, \n                                  {'op': 'copy', 'params': {'target': 'all_objects', 'delta': (0,1)}}]},\n            'delete_all': {'task': {'train': [{'input': [[1]], 'output': [[0]]}]},\\\n                           'arch': [{'op': 'find', 'params': {}}, \n                                    {'op': 'delete', 'params': {'target': 'all_objects'}}]},\n            'draw_path': {'task': {'train': [{'input': [[1,0,1]], 'output': [[1,5,1]]}]},\\\n                          'arch': [{'op': 'find', 'params': {}}, \n                                   {'op': 'draw_path', 'params': {'target': 'all_objects', 'color': 5}}]},\n            'map_delete': {'task': {'train': [{'input': [[1,2,1]], 'output': [[0,2,0]]}]},\n                           'arch': [{'op': 'find', 'params': {'color': 1}}, \n                                    {'op': 'map', 'params': {'target': 'last_result', 'program': [{'op': 'delete', 'params': {'target': 'this'}}]}}]},\n            'move_largest': {'task': {'train': [{'input': np.array([[1,0],[2,2]]), 'output': np.array([[1,0],[0,2,2]])}]},\n                             'arch': [{'op': 'find', 'params': {}}, \n                                      {'op': 'get_largest', 'params': {'target': 'all_objects'}}, \n                                      {'op': 'move', 'params': {'target': 'last_result', 'delta': (0,1)}}]},\n            'delete_smallest': {'task': {'train': [{'input': np.array([[1,0],[2,2]]), 'output': np.array([[0,0],[2,2]])}]},\n                                'arch': [{'op': 'find', 'params': {}}, \n                                         {'op': 'get_smallest', 'params': {'target': 'all_objects'}}, \n                                         {'op': 'delete', 'params': {'target': 'last_result'}}]},\n        }\n\n        # 3. Build HPN Level 1 (The \"Playbook\")\n        playbook_vectors = []\n        playbook_programs = []\n        \n        for name, data in QUIZ_TASKS.items():\n            try:\n                v = fingerprinter.fingerprint(data['task'])\n                playbook_vectors.append(v)\n                playbook_programs.append(data['arch'])\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è  Bootcamp HPN-L1 failed for {name}: {e}\")\n                continue\n        \n        final_playbook_vectors = None\n        if playbook_vectors:\n            final_playbook_vectors = np.stack(playbook_vectors)\n\n        # 4. Build HPN Level 2 (The \"Grammar\")\n        hpn_counts = defaultdict(lambda: defaultdict(int))\n        for program_ast in CURRICULUM:\n            prev_op = 'START'\n            for instruction in program_ast:\n                op_name = instruction['op']\n                hpn_counts[prev_op][op_name] += 1\n                prev_op = op_name\n        \n        hpn_grammar = defaultdict(dict)\n        for prev_op, next_ops in hpn_counts.items():\n            total_transitions = sum(next_ops.values())\n            if total_transitions > 0:\n                for next_op, count in next_ops.items():\n                    hpn_grammar[prev_op][next_op] = count / total_transitions\n        \n        return final_playbook_vectors, playbook_programs, hpn_grammar\n\n    def run_pre_computation(self, all_known_tasks, all_known_solutions):\n        \"\"\"\n        *** v2.0: \"GAME GENIE\" (On-the-fly Training) ***\n        This loop now uses the fast v2.0 synthesizer and should\n        finally succeed in caching programs.\n        \"\"\"\n        print(\"\\n--- üß† EXECUTING PRE-COMPUTATION: 'GAME GENIE' v2.0 LTM TRAINING ---\")\n        if not all_known_tasks or not all_known_solutions:\n            print(\"  ‚ö†Ô∏è  Missing training/eval data. Skipping 'Game Genie' training.\")\n            return\n\n        profiler.start(\"GameGenie_LTMv2_Training\")\n        self.metric_logger.write_header(self.LTM_LOG_COLUMNS)\n        \n        fingerprinter = self.solver_toolbox.fingerprinter\n        total_tasks_to_train = len(all_known_tasks)\n        tasks_cached = 0\n        \n        # --- Dynamic Budget Calculation ---\n        total_ltm_budget = self.config.total_time_budget * self.config.LTM_BUDGET_PERCENT\n        \n        if self.config.DIAGNOSTIC_RUN:\n             diag_scaled_budget = 0.0\n             if self.LTM_NUM_TASKS > 0:\n                 diag_scaled_budget = (total_ltm_budget / self.LTM_NUM_TASKS) * total_tasks_to_train\n             \n             min_budget_seconds = self.config.DIAGNOSTIC_MIN_RUNTIME_MINUTES * 60\n             total_ltm_budget = max(diag_scaled_budget, min_budget_seconds)\n             \n             print(f\"  *** ‚ö†Ô∏è  DIAGNOSTIC MODE: Training on {total_tasks_to_train} tasks... ***\")\n             print(f\"  Scaled LTM Budget: {total_ltm_budget / 60:.2f} minutes ({self.config.DIAGNOSTIC_MIN_RUNTIME_MINUTES:.0f}-min floor enforced)\")\n        else:\n             print(f\"  Running 'Game Genie' LTM training on {total_tasks_to_train} tasks.\")\n             print(f\"  Total LTM Budget: {total_ltm_budget / 3600:.2f} hours.\")\n        \n        \n        # --- Phase 0 - Triage ALL tasks first ---\n        print(\"  Phase 0: Triaging all tasks for curriculum...\")\n        profiler.start(\"GameGenie.TriageAll\")\n        sorted_task_list = []\n        for task_id, task_data in all_known_tasks.items():\n            if task_id not in all_known_solutions:\n                continue\n            difficulty = estimate_task_difficulty(task_data)\n            tier = 'easy' if difficulty < 7.0 else ('medium' if difficulty < 18.0 else 'hard')\n            sorted_task_list.append((task_id, task_data, difficulty, tier))\n        \n        sorted_task_list.sort(key=lambda x: x[2]) # Sort by difficulty\n        profiler.end(\"GameGenie.TriageAll\")\n        \n        \n        # --- \"Shaving\" Budget Calculation ---\n        num_punt_tasks = 10\n        num_standard_tasks = max(0, total_tasks_to_train - num_punt_tasks)\n        \n        punt_timeout = self.config.PUNT_TASK_BUDGET_SECONDS # 60.0s\n        total_punt_cost = num_punt_tasks * punt_timeout # 10 * 60s = 600s\n        \n        remaining_budget = total_ltm_budget - total_punt_cost\n        \n        if num_standard_tasks > 0:\n            standard_timeout = remaining_budget / num_standard_tasks\n        else:\n            standard_timeout = 0\n        \n        if standard_timeout < 1.0:\n            if remaining_budget > 0:\n                 print(f\"  ‚ö†Ô∏è  WARNING: Punt tasks consumed most of LTM budget. \"\n                       f\"Standard tasks will have {standard_timeout:.2f}s timeout.\")\n            standard_timeout = max(1.0, standard_timeout) # 1s minimum\n            \n        \n        tasks_to_punt = sorted_task_list[:num_punt_tasks]\n        tasks_to_standard_solve = sorted_task_list[num_punt_tasks:]\n\n        print(f\"  Budgeting: Standard Timeout = {standard_timeout:.2f}s | Punt Timeout = {punt_timeout:.2f}s\")\n        \n        \n        # --- Phase A - \"Punt\" Tasks (FORCED v2.0 SLOW BRAIN) ---\n        print(f\"\\n  --- Phase A: Solving {len(tasks_to_punt)} Easiest Tasks ({punt_timeout:.0f}s Budget) ---\")\n        tasks_processed = 0\n        try:\n            for (task_id, task_data, difficulty, tier) in tasks_to_punt:\n                task_start_time = time.time()\n                status = \"Fail.Unknown\"\n                program_cached = False\n                \n                try:\n                    ground_truth_outputs = all_known_solutions[task_id]\n                    # --- v2.0: This now calls the *fast* synthesizer ---\n                    (validated_program_ast, status) = self._find_ground_truth_program(\n                        task_id, task_data, ground_truth_outputs, \n                        timeout=punt_timeout, \n                        is_punt_task=True # Forces Slow Brain\n                    )\n                    \n                    if validated_program_ast is not None:\n                        v_delta = fingerprinter.fingerprint(task_data)\n                        self._ltm_vectors_list.append(v_delta)\n                        self.ltm_programs.append(validated_program_ast)\n                        program_cached = True\n                        tasks_cached += 1\n                \n                except Exception as e:\n                    status = f\"Fail.Crash.{type(e).__name__}\"\n                finally:\n                    time_taken = time.time() - task_start_time\n                    self.metric_logger.log({\n                        'columns_order': self.LTM_LOG_COLUMNS, 'phase': \"GameGenie.Punt\",\n                        'task_id': task_id, 'task_tier': tier, 'status': status,\n                        'time_taken_s': f\"{time_taken:.3f}\", 'program_cached': program_cached\n                    })\n                    tasks_processed += 1\n                    print(f\"  Punt Task {tasks_processed}/{len(tasks_to_punt)}: {task_id} \"\n                          f\"({tier}) -> {status}. (Cached: {program_cached})\")\n\n        except Exception as e:\n            print(f\"  üî•üî•üî• CRASH during LTM PUNT phase: {e}\")\n\n\n        # --- Phase B - \"Standard\" Tasks (Fast -> Slow Fallback) ---\n        print(f\"\\n  --- Phase B: Solving {len(tasks_to_standard_solve)} Remaining Tasks ({standard_timeout:.2f}s Budget) ---\")\n        try:\n            for (task_id, task_data, difficulty, tier) in tasks_to_standard_solve:\n                task_start_time = time.time()\n                status = \"Fail.Unknown\"\n                program_cached = False\n                \n                try:\n                    ground_truth_outputs = all_known_solutions[task_id]\n                    # --- v2.0: This now calls the *fast* synthesizer on fallback ---\n                    (validated_program_ast, status) = self._find_ground_truth_program(\n                        task_id, task_data, ground_truth_outputs, \n                        timeout=standard_timeout, \n                        is_punt_task=False # Tries Fast Brain first\n                    )\n                    \n                    if validated_program_ast is not None:\n                        v_delta = fingerprinter.fingerprint(task_data)\n                        self._ltm_vectors_list.append(v_delta)\n                        self.ltm_programs.append(validated_program_ast)\n                        program_cached = True\n                        tasks_cached += 1\n                \n                except Exception as e:\n                    status = f\"Fail.Crash.{type(e).__name__}\"\n                finally:\n                    time_taken = time.time() - task_start_time\n                    self.metric_logger.log({\n                        'columns_order': self.LTM_LOG_COLUMNS, 'phase': \"GameGenie.Standard\",\n                        'task_id': task_id, 'task_tier': tier, 'status': status,\n                        'time_taken_s': f\"{time_taken:.3f}\", 'program_cached': program_cached\n                    })\n                    tasks_processed += 1\n                    \n                    if (tasks_processed % 10 == 0) or (tasks_processed == total_tasks_to_train):\n                        print(f\"  LTM Training Progress... Task {tasks_processed}/{total_tasks_to_train} processed. \"\n                              f\"({tasks_cached} programs cached)\")\n\n        except Exception as e:\n            print(f\"  üî•üî•üî• CRASH during LTM STANDARD phase: {e}\")\n            import traceback\n            traceback.print_exc()\n\n        # --- Finalize ---\n        if self._ltm_vectors_list:\n            self.ltm_vectors = np.stack(self._ltm_vectors_list)\n            print(f\"\\n  ‚úÖ 'Game Genie' LTM v2.0 Training complete.\")\n            print(f\"  Cached {self.ltm_vectors.shape[0]} programs in a \"\n                  f\"({self.ltm_vectors.shape[0]} x {self.ltm_vectors.shape[1]}) vector database.\")\n        else:\n            print(f\"\\n  ‚ö†Ô∏è  'Game Genie' LTM v2.0 Training FAILED. No programs were cached.\")\n\n        profiler.end(\"GameGenie_LTMv2_Training\")\n\n    def _find_ground_truth_program(self, task_id: str, task_data: Dict, \n                                   ground_truth_outputs: List[Dict], timeout: float,\n                                   is_punt_task: bool = False\n                                   ) -> Tuple[Optional[List[Dict]], str]:\n        \"\"\"\n        v2.0: This function's logic is unchanged, but its component calls\n        (to fast_solver and slow_solver) are now to the v2.0 components.\n        The `slow_solver.solve()` call is now computationally tractable.\n        \"\"\"\n        profiler.start(f\"v2.GameGenie.find_program.{task_id}\")\n        \n        program_ast = None\n        rule_name = \"Fail.Unknown\"\n\n        if not is_punt_task:\n            # --- 1. Call \"Fast Brain\" (The \"Architect\") ---\n            fast_brain_timeout = min(timeout, 2.0)\n            fast_solver = self.solver_toolbox.heuristic_solver\n            # This uses the v2.0 interpreter for validation, which is correct.\n            program_ast, rule_name = fast_solver.solve(task_data, timeout=fast_brain_timeout) \n            \n            if program_ast is not None:\n                holdout_inputs = task_data.get('test', [])\n                if self._validate_on_holdout(program_ast, holdout_inputs, ground_truth_outputs):\n                    profiler.end(f\"v2.GameGenie.find_program.{task_id}\")\n                    return program_ast, rule_name # e.g., \"Playbook.Success\"\n                else:\n                    rule_name = \"Playbook.Fail.Holdout\"\n                    program_ast = None # Clear it, force slow brain\n        \n        # --- 2. Call \"Schooled Slow Brain\" (The \"Searcher\") ---\n        if program_ast is None:\n            # *** THIS IS THE KEY v2.0 FIX ***\n            # This now calls the fast, symbolic v2.0 synthesizer.\n            slow_solver = self.solver_toolbox.synthesizer \n            program_ast, rule_name = slow_solver.solve(task_data, timeout=timeout) \n        \n        if program_ast is None:\n            profiler.end(f\"v2.GameGenie.find_program.{task_id}\")\n            return None, rule_name # e.g., \"Synthesizer.Fail.MaxDepth\"\n            \n        # --- 3. Final Validation ---\n        holdout_inputs = task_data.get('test', [])\n        \n        if self._validate_on_holdout(program_ast, holdout_inputs, ground_truth_outputs):\n            profiler.end(f\"v2.GameGenie.find_program.{task_id}\")\n            return program_ast, rule_name # e.g., \"Synthesizer.Success.d{N}\"\n        else:\n            profiler.end(f\"v2.GameGenie.find_program.{task_id}\")\n            return None, \"Synthesizer.Fail.Holdout\"\n\n    def _validate_on_holdout(self, program_ast: List[Dict], \n                               test_inputs: List[Dict], \n                               ground_truth_solutions: List[Dict]) -> bool:\n        \"\"\" Uses the v2.0 interpreter's public `run` method. \"\"\"\n        interpreter = self.solver_toolbox.interpreter\n        \n        if len(test_inputs) != len(ground_truth_solutions):\n            return False \n        if not test_inputs:\n            return True # No test cases to fail on\n\n        try:\n            for i in range(len(test_inputs)):\n                inp_grid = np.array(test_inputs[i]['input'])\n                expected_output_grid = np.array(ground_truth_solutions[i]['output'])\n                \n                # interpreter.run is the v2.0 method that handles\n                # perceive-once and symbolic-execution.\n                predicted_grid = interpreter.run(program_ast, inp_grid)\n                \n                if not np.array_equal(predicted_grid, expected_output_grid):\n                    return False\n            return True\n        except Exception as e:\n            return False\n\n    def query_ltm_cache(self, novel_fingerprint: np.ndarray) -> List[Tuple[List[Dict], str, float]]:\n        \"\"\" Performs a k-Nearest Neighbor (k-NN) search \"\"\"\n        profiler.start(\"v2.query_ltm_cache\")\n        \n        results = []\n        if self.ltm_vectors is None or novel_fingerprint is None or len(self.ltm_programs) == 0:\n            profiler.end(\"v2.query_ltm_cache\")\n            return results\n            \n        try:\n            distances = np.linalg.norm(self.ltm_vectors - novel_fingerprint, axis=1)\n            k = self.config.LTM_CACHE_K\n            nearest_indices = np.argsort(distances)[:k]\n            \n            for i in nearest_indices:\n                program_ast = self.ltm_programs[i]\n                dist = distances[i]\n                name = f\"LTM_Abduction_k{i}_dist{dist:.2f}\"\n                results.append( (program_ast, name, dist) )\n                    \n        except Exception as e:\n            pass # Fail silently on LTM query\n            \n        profiler.end(\"v2.query_ltm_cache\")\n        return results\n\n    def update_ltm_cache(self, fingerprint: np.ndarray, program_ast: List[Dict]):\n        \"\"\" Performs \"Online Learning\". \"\"\"\n        profiler.start(\"v2.update_ltm_cache\")\n        try:\n            self._ltm_vectors_list.append(fingerprint)\n            self.ltm_vectors = np.stack(self._ltm_vectors_list)\n            self.ltm_programs.append(program_ast)\n        except Exception as e:\n            pass # Fail silently\n        profiler.end(\"v2.update_ltm_cache\")\n\n    def run_triage_phase(self, tasks_to_analyze: Dict[str, Dict]) -> Counter:\n        \"\"\"\n        Executes \"Phase 1 Triage\" on the (unknown) TEST SET.\n        (v2.0: No changes needed, this logic is sound.)\n        \"\"\"\n        print(\"\\n--- üß† EXECUTING PHASE 1: HEURISTIC TRIAGE (on test set) ---\")\n        if not tasks_to_analyze:\n            print(\"  ‚ùå No tasks to analyze. Triage skipped.\")\n            return Counter()\n            \n        profiler.start(\"Phase1_Triage_Full\")\n        \n        tasks_to_actually_triage = tasks_to_analyze\n        if self.config.DIAGNOSTIC_RUN:\n            print(f\"  *** ‚ö†Ô∏è  DIAGNOSTIC MODE: Triaging {self.config.DIAGNOSTIC_SAMPLE_SIZE} tasks... ***\")\n            task_keys_to_triage = list(tasks_to_analyze.keys())[:self.config.DIAGNOSTIC_SAMPLE_SIZE]\n            tasks_to_actually_triage = {k: tasks_to_analyze[k] for k in task_keys_to_triage}\n        \n        for task_id, task_data in tasks_to_actually_triage.items():\n            # This calls the Analyzer from the v2.0-built toolbox\n            profile = self.solver_toolbox.analyzer.analyze(task_data, task_id)\n            self.task_profiles[task_id] = profile\n            \n        profiler.end(\"Phase1_Triage_Full\")\n        \n        stats = Counter([p.difficulty_tier for p in self.task_profiles.values()])\n        print(f\"  ‚úÖ Triage complete. Task profile generated for all {len(self.task_profiles)} tasks.\")\n        print(f\"  Triage Stats: {stats['easy']} Easy | {stats['medium']} Medium | {stats['hard']} Hard\")\n        return stats\n\n    def allocate_time_budgets(self, total_solve_budget: float, tier_counts: Counter):\n        \"\"\"\n        (v2.0: No changes needed, this logic is sound.)\n        \"\"\"\n        print(\"\\n--- ‚è≥ Allocating time budgets (ŒîH Modulation) ---\")\n        \n        total_abstraction_budget = total_solve_budget * self.config.abstraction_pass_time_ratio\n        total_reasoning_budget = total_solve_budget * self.config.reasoning_pass_time_ratio\n        \n        print(f\"  Abstraction Pass (ŒîH-) Total Budget: {total_abstraction_budget/3600:.2f} hours\")\n        print(f\"  Reasoning Pass (ŒîH+) Total Budget: {total_reasoning_budget/3600:.2f} hours\")\n\n        total_tasks = len(self.task_profiles)\n        if total_tasks == 0:\n            print(\"  ‚ùå No tasks found. Cannot allocate time.\")\n            return\n\n        # --- 1. Identify the 9 \"Punt\" Tasks (3 easy, 3 med, 3 hard) ---\n        sorted_profiles = sorted(self.task_profiles.values(), key=lambda p: p.difficulty_score)\n        \n        punt_tasks_easy = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'easy'][:3]\n        punt_tasks_medium = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'medium'][:3]\n        punt_tasks_hard = [p.task_id for p in sorted_profiles if p.difficulty_tier == 'hard'][:3]\n        \n        punt_task_set = set(punt_tasks_easy + punt_tasks_medium + punt_tasks_hard)\n        print(f\"  Identified {len(punt_task_set)} tasks for 2x budget 'punt' (60s).\")\n\n        # --- 2. Calculate \"Shaved\" Budget (MDMP) ---\n        weights = {'easy': 1, 'medium': 3, 'hard': 6}\n        punt_timeout = self.config.PUNT_TASK_BUDGET_SECONDS # 60.0s\n        \n        punt_cost_reasoning = len(punt_task_set) * punt_timeout\n        punt_cost_abstraction = len(punt_task_set) * 10.0 # 10s for LTM/HPN check\n        \n        remaining_budget_reasoning = total_reasoning_budget - punt_cost_reasoning\n        remaining_budget_abstraction = total_abstraction_budget - punt_cost_abstraction\n        \n        remaining_weighted_units = 0\n        for task_id, profile in self.task_profiles.items():\n            if task_id not in punt_task_set:\n                remaining_weighted_units += weights.get(profile.difficulty_tier, 1)\n\n        if remaining_weighted_units <= 0: remaining_weighted_units = 1\n        \n        unit_time_abstraction = remaining_budget_abstraction / remaining_weighted_units\n        unit_time_reasoning = remaining_budget_reasoning / remaining_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:41:15.101454Z","iopub.execute_input":"2025-11-09T12:41:15.101846Z","iopub.status.idle":"2025-11-09T12:41:15.166091Z","shell.execute_reply.started":"2025-11-09T12:41:15.101812Z","shell.execute_reply":"2025-11-09T12:41:15.164956Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 9 (v2.0) RSC Controller & LTM Trainer\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Cell 10\n################################################################################\n#\n# üåä‚öõÔ∏è LUCIDORCA ULTIMATE SOLVER - v2.0 REBUILD\n#\n# Cell 10: Main Inference, Validation, and Save (v2.0)\n#\n# *** v2.0 REBUILD (DSS/CWM Integration) ***\n#\n# 1. `_validate_program_on_train`: This helper function is refactored\n#    to use the new `v2.0 interpreter.run()` method. This is critical\n#    for the LTM Abduction (k-NN) \"sanity check\" phase.\n# 2. Main Inference Loop: The loop now correctly pulls the v2.0-compliant\n#    `interpreter`, `fast_brain_solver`, and `slow_brain_solver`\n#    from the `controller.solver_toolbox`.\n# 3. Final Application: The `interpreter.run(final_program_X, ...)`\n#    calls at the end of the loop now correctly use the v2.0 (DSS/CWM)\n#    execution engine to generate the final submission grids.\n#\n# *** v2.0 FIX (NameError) ***\n# 1. Moved sections 5, 6, and 7 (Validation, Save, Summary) *inside* the\n#    main `else` block. This prevents the `NameError` by ensuring\n#    this code only runs if the `controller` from Cell 9 was\n#    successfully loaded.\n#\n################################################################################\n\nprint(\"=\"*70)\nprint(f\"üåä‚öõÔ∏è LucidOrca Solver: Cell 10 (v2.0) - Main Inference Loop\")\n\n# --- 1. Setup Execution ---\n\nsubmission = {}\nDEFAULT_PROGRAM_AST = [] # An empty program (returns the input grid)\n\ndef _generate_variation_grid(grid: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Generates a *different* grid to be used for `attempt_2`\n    if both solver passes return the *exact same* grid.\n    \"\"\"\n    if grid is None or grid.size == 0:\n        return np.array([[0]])\n\n    try:\n        variation_1 = np.rot90(grid, 1)\n        if not np.array_equal(variation_1, grid):\n            return variation_1\n        \n        variation_2 = np.fliplr(grid)\n        if not np.array_equal(variation_2, grid):\n            return variation_2\n            \n        return np.rot90(grid, 2)\n        \n    except Exception:\n        return np.array([[0]])\n\n\ndef _validate_program_on_train(program_ast: List[Dict], \n                             task_data: Dict, \n                             interpreter: SymbolicProgramInterpreter) -> bool:\n    \"\"\"\n    *** v2.0 REFACTOR ***\n    Validates a \"best guess\" program (from LTM) against the\n    novel task's *own* training examples using the v2.0 interpreter.\n    \"\"\"\n    if program_ast is None:\n        return False\n        \n    examples = task_data.get('train', [])\n    if not examples:\n        return False # Cannot validate\n\n    try:\n        for ex in examples:\n            inp_grid = np.array(ex['input'])\n            expected_output_grid = np.array(ex['output'])\n            \n            # Use the v2.0 interpreter's public `run` method\n            predicted_grid = interpreter.run(program_ast, inp_grid)\n            \n            if not np.array_equal(predicted_grid, expected_output_grid):\n                return False\n        return True # Passed all examples\n    except Exception:\n        return False\n\n\n# Check that the controller and tasks from previous cells are loaded\nif 'controller' not in locals() or 'test_tasks' not in locals() or not test_tasks:\n    print(\"‚ùå CRITICAL ERROR: `controller` or `test_tasks` not found.\")\n    print(\"   Please re-run Cell 8 and Cell 9 before this one.\")\n    submission = {} # Define for safety if \"Run All\" is used\n    \nelse:\n    # --- 2. Get All v2.0 AGI Components from Controller ---\n    \n    solver_toolbox = controller.solver_toolbox\n    \n    # These are now all the v2.0-compliant components\n    interpreter = solver_toolbox.interpreter\n    fast_brain_solver = solver_toolbox.heuristic_solver # \"The Architect\"\n    slow_brain_solver = solver_toolbox.synthesizer      # \"The Searcher\"\n    \n    task_profiles = controller.task_profiles\n    time_allocations = controller.time_allocations\n    \n    total_solve_budget = solve_budget \n    main_loop_start_time = time.time()\n\n    sorted_task_ids = list(task_profiles.keys())\n    total_tasks_count = len(sorted_task_ids)\n    \n    print(f\"  Starting main v2.0 inference loop for {total_tasks_count} tasks...\")\n    if CONFIG.DIAGNOSTIC_RUN:\n        print(f\"  *** ‚ö†Ô∏è  DIAGNOSTIC MODE: Inference run is limited to {total_tasks_count} tasks. ***\")\n    print(f\"  Total Solve Budget: {total_solve_budget/3600:.2f} hours\")\n    print(\"=\"*70)\n\n    # --- HOTFIX 9: Write new header for Inference phase ---\n    controller.metric_logger.write_header(controller.INFERENCE_LOG_COLUMNS)\n\n    # --- 3. Main Execution Loop (v2.0 Inference) ---\n    \n    for i, task_id in enumerate(sorted_task_ids):\n        task_start_time = time.time()\n        \n        task_data = test_tasks[task_id]\n        profile = task_profiles[task_id]\n        \n        elapsed_total = time.time() - main_loop_start_time\n        if elapsed_total > total_solve_budget:\n            print(f\"\\n‚è±Ô∏è  MASTER TIME BUDGET EXCEEDED. Stopping solve loop.\")\n            print(f\"  Completed {i}/{total_tasks_count} tasks.\")\n            break\n            \n        print(f\"\\n--- [ {i+1}/{total_tasks_count} ] Solving Task: {task_id} (Tier: {profile.difficulty_tier}, Basin: {profile.basin}) ---\")\n        \n        program_LTM = None # \"Abstraction\" (LTM k-NN)\n        program_Heuristic = None # \"Architect\" (HPN Playbook)\n        program_DeepSearch = None # \"Searcher\" (v2.0 Symbolic Search)\n\n        ltm_status = \"Miss\"\n        heuristic_status = \"Fail.NotRun\"\n        reasoning_status = \"Fail.NotRun\"\n        \n        # --- C. Execute Phase 2: Abstraction (LTM k-NN) ---\n        profiler.start(f\"v2.Phase2_LTM_Query.{task_id}\")\n        \n        novel_fingerprint = profile.delta_fingerprint\n        \n        if novel_fingerprint is None:\n            print(\"  ‚ö†Ô∏è  Task fingerprinting failed. Skipping LTM query.\")\n            ltm_status = \"Fail.NoFingerprint\"\n        else:\n            print(\"  Querying LTM-v2.0 (k-NN)...\")\n            candidates = controller.query_ltm_cache(novel_fingerprint)\n            \n            if candidates:\n                print(f\"  LTM-v2.0 returned {len(candidates)} candidates. Starting Sanity Check...\")\n                for (program_ast, rule_name, dist) in candidates:\n                    # *** v2.0: This now uses the correct interpreter ***\n                    if _validate_program_on_train(program_ast, task_data, interpreter):\n                        print(f\"  ‚úÖ Abduction PASSED Sanity Check! (Found {rule_name})\\n\")\n                        program_LTM = program_ast\n                        ltm_status = \"Hit\"\n                        # SOTA Feature 5 (PAR): This is where we would also do\n                        # \"online learning\" / refinement.\n                        controller.update_ltm_cache(novel_fingerprint, program_LTM)\n                        break \n                if program_LTM is None:\n                    print(\"  ‚ö†Ô∏è  LTM Abduction FAILED Sanity Check on all candidates.\")\n                    ltm_status = \"Fail.SanityCheck\"\n            else:\n                print(\"  LTM-v2.0 Cache Miss. No similar task found.\")\n                ltm_status = \"Miss\"\n        profiler.end(f\"v2.Phase2_LTM_Query.{task_id}\")\n\n        # --- D. Execute Phase 3a: Heuristics (\"Fast Brain\") ---\n        if program_LTM is None:\n            profiler.start(f\"v2.Phase3a_HeuristicSolver.{task_id}\")\n            print(\"  Executing Phase 3a (ŒîH-) Fast-Brain Architect...\")\n            \n            abstraction_budget = time_allocations['abstraction_per_task'][task_id]\n            # This calls the v2.0-compliant fast_brain_solver\n            (program_ast, rule_name) = fast_brain_solver.solve(task_data, timeout=abstraction_budget)\n            \n            if program_ast is not None:\n                program_Heuristic = program_ast\n                heuristic_status = rule_name # e.g., \"Playbook.Success\"\n                print(f\"  ‚úÖ Phase 3a (ŒîH-) Fast-Brain SUCCESS: [{heuristic_status}]\")\n            else:\n                heuristic_status = rule_name # e.g., \"Playbook.Fail.NoMatch\"\n                print(f\"  ‚ö†Ô∏è  Phase 3a (ŒîH-) Fast-Brain FAILED: [{heuristic_status}]\")\n            profiler.end(f\"v2.Phase3a_HeuristicSolver.{task_id}\")\n\n        # --- E. Execute Phase 3b: Reasoning (\"Slow Brain\") ---\n        if program_LTM is None and program_Heuristic is None:\n            profiler.start(f\"v2.Phase3b_DeepSearch.{task_id}\")\n            print(\"  Executing Phase 3b (ŒîH+) 'Schooled Mind' Search...\")\n            \n            reasoning_budget = time_allocations['reasoning_per_task'][task_id]\n            # This calls the fast v2.0 symbolic synthesizer\n            (program_ast, rule_name) = slow_brain_solver.solve(task_data, timeout=reasoning_budget)\n            \n            if program_ast is not None:\n                program_DeepSearch = program_ast\n                reasoning_status = rule_name # e.g., \"Synthesizer.Success.d3\"\n                print(f\"  ‚úÖ Phase 3b (ŒîH+) 'Schooled Mind' SUCCESS: [{reasoning_status}]\")\n            else:\n                reasoning_status = rule_name # e.g., \"Synthesizer.Fail.MaxDepth\"\n                print(f\"  ‚ö†Ô∏è  Phase 3b (ŒîH+) 'Schooled Mind' FAILED: [{reasoning_status}]\")\n            profiler.end(f\"v2.Phase3b_DeepSearch.{task_id}\")\n\n\n        # --- F. RSC Arbiter: Select Final Programs ---\n        final_program_1 = program_LTM or program_Heuristic or program_DeepSearch or DEFAULT_PROGRAM_AST\n        final_program_2 = program_DeepSearch or program_Heuristic or program_LTM or DEFAULT_PROGRAM_AST\n        \n        final_program_source = \"Fallback\"\n        if program_LTM: final_program_source = \"LTM\"\n        elif program_Heuristic: final_program_source = \"Heuristic\"\n        elif program_DeepSearch: final_program_source = \"DeepSearch\"\n        \n        \n        # --- G. CRITICAL: Apply Programs to ALL Test Cases ---\n        task_solutions = [] \n        num_expected_outputs = len(task_data.get('test', []))\n        \n        if num_expected_outputs == 0:\n            print(f\"  ‚ùå Task {task_id} has no test cases. Skipping.\")\n            continue\n            \n        print(f\"  Applying programs to {num_expected_outputs} test case(s)...\")\n        \n        for test_case_index in range(num_expected_outputs):\n            try:\n                test_input_grid = np.array(task_data['test'][test_case_index]['input'])\n            except Exception:\n                test_input_grid = np.array([[0]]) \n            \n            try:\n                # Use the v2.0 interpreter.run() method\n                grid_1 = interpreter.run(final_program_1, test_input_grid)\n            except Exception:\n                grid_1 = test_input_grid\n                \n            try:\n                # Use the v2.0 interpreter.run() method\n                grid_2 = interpreter.run(final_program_2, test_input_grid)\n            except Exception:\n                grid_2 = test_input_grid\n                \n            if np.array_equal(grid_1, grid_2):\n                grid_2 = _generate_variation_grid(grid_1)\n                \n            task_solutions.append({\n                \"attempt_1\": grid_1.tolist(),\n                \"attempt_2\": grid_2.tolist()\n            })\n\n        # --- H. Store Final Solutions for this Task ---\n        submission[task_id] = task_solutions\n        task_time = time.time() - task_start_time\n        print(f\"  ‚û°Ô∏è  Task {task_id} finished in {task_time:.2f}s. Stored {len(task_solutions)} solutions.\")\n\n        # --- HOTFIX 9: Log Inference Metrics ---\n        log_data = {\n            'columns_order': controller.INFERENCE_LOG_COLUMNS,\n            'phase': \"Inference\",\n            'task_id': task_id,\n            'task_tier': profile.difficulty_tier,\n            'basin': profile.basin,\n            'ltm_status': ltm_status,\n            'reasoning_status': f\"{heuristic_status} | {reasoning_status}\",\n            'final_program': final_program_source,\n            'time_taken_s': f\"{task_time:.3f}\"\n        }\n        controller.metric_logger.log(log_data)\n\n\n    # --- 4. Final Summary of Loop ---\n    \n    total_solve_time = time.time() - main_loop_start_time\n    print(\"\\n\" + \"=\"*70)\n    print(\"‚úÖ MAIN INFERENCE LOOP COMPLETE\")\n    print(f\"  Total tasks processed: {len(submission)} / {total_tasks_count}\")\n    print(f\"  Total Solve Time: {total_solve_time / 60:.2f} minutes\")\n    profiler.print_summary()\n\n\n    # --- 5. Final Validation & Sanitization ---\n    # *** MOVED INSIDE THE ELSE BLOCK ***\n    print(\"\\n\" + \"=\"*70)\n    print(\"üåä‚öõÔ∏è Final Validation & Sanitization Pass\")\n    print(\"=\"*70)\n    \n    def generate_compliant_fallback(task_id: str, task_data: Dict) -> List[Dict]:\n        \"\"\"\n        Generates a compliant fallback solution (copies input)\n        for a given task_id.\n        \"\"\"\n        try:\n            fallback_grid = np.array(task_data['test'][0]['input'])\n        except Exception:\n            fallback_grid = np.array([[0]]) \n    \n        num_expected_outputs = len(task_data.get('test', []))\n        if num_expected_outputs == 0:\n            return [] \n    \n        all_test_solutions = []\n        for test_case_index in range(num_expected_outputs):\n            try:\n                input_grid = np.array(task_data['test'][test_case_index]['input'])\n            except Exception:\n                input_grid = fallback_grid \n                \n            attempt_1_grid = input_grid\n            attempt_2_grid = _generate_variation_grid(attempt_1_grid)\n            \n            all_test_solutions.append({\n                \"attempt_1\": attempt_1_grid.tolist(),\n                \"attempt_2\": attempt_2_grid.tolist()\n            })\n        \n        return all_test_solutions\n    \n    \n    validation_passed = True\n    tasks_overwritten = 0\n    tasks_missing = 0\n    \n    if not test_tasks:\n        print(\"‚ùå No test tasks loaded. Cannot validate or save.\")\n        validation_passed = False\n    else:\n        print(f\"  Validating submission against all {len(test_tasks)} required tasks...\")\n        \n        for task_id, task_data in test_tasks.items():\n            \n            num_expected_outputs = len(task_data.get('test', []))\n            if num_expected_outputs == 0:\n                continue \n    \n            is_valid = True \n    \n            if task_id not in submission:\n                tasks_missing += 1\n                is_valid = False\n                \n            else:\n                task_outputs_list = submission[task_id]\n                \n                if not isinstance(task_outputs_list, list): is_valid = False\n                elif len(task_outputs_list) != num_expected_outputs: is_valid = False\n                else:\n                    for item in task_outputs_list:\n                        if not isinstance(item, dict): is_valid = False; break\n                        if \"attempt_1\" not in item or \"attempt_2\" not in item: is_valid = False; break\n                        if not isinstance(item['attempt_1'], list) or not isinstance(item['attempt_2'], list): is_valid = False; break\n    \n            if not is_valid:\n                if task_id in submission:\n                    print(f\"  ‚ùå Task {task_id} has FORMAT ERROR. Overwriting with compliant fallback.\")\n                    tasks_overwritten += 1\n                \n                submission[task_id] = generate_compliant_fallback(task_id, task_data)\n                validation_passed = False\n    \n    if tasks_missing > 0:\n        print(f\"\\n! VALIDATION WARNING: {tasks_missing} tasks were missing and replaced with fallbacks.\")\n    if tasks_overwritten > 0:\n        print(f\"\\n! VALIDATION ERROR: {tasks_overwritten} tasks had format errors and were overwritten.\")\n    if validation_passed and tasks_missing == 0 and tasks_overwritten == 0:\n        print(\"\\n‚úÖ‚úÖ‚úÖ SUBMISSION FORMAT IS 100% VALID! ‚úÖ‚úÖ‚úÖ\")\n    else:\n        print(\"\\n‚ö†Ô∏è  Submission contains fallbacks or errors, but is now 100% format-compliant.\")\n    \n    \n    # --- 6. Save the Sanitized Submission ---\n    # *** MOVED INSIDE THE ELSE BLOCK ***\n    OUTPUT_PATH = Path(\"/kaggle/working/submission.json\")\n    print(f\"\\nüíæ Saving sanitized submission to: {OUTPUT_PATH}\")\n    \n    try:\n        with open(OUTPUT_PATH, 'w') as f:\n            json.dump(submission, f, separators=(',', ':'))\n    \n        file_size = OUTPUT_PATH.stat().st_size\n        print(f\"   ‚úÖ Saved {len(submission)} tasks ({file_size/1024:.1f} KB)\")\n    \n    except Exception as e:\n        print(f\"‚ùå CRITICAL ERROR: FAILED TO SAVE SUBMISSION: {e}\")\n    \n    # --- 7. Final Notebook Summary ---\n    # *** MOVED INSIDE THE ELSE BLOCK ***\n    total_notebook_time = time.time() - notebook_start_time\n    print(\"\\n\" + \"=\"*70)\n    print(\"üèÜüèÅ NOTEBOOK EXECUTION COMPLETE üèÅüèÜ\")\n    print(\"=\"*70)\n    print(f\"  Total Tasks in Submission: {len(submission)} / {len(test_tasks)}\")\n    print(f\"  Total Notebook Runtime: {total_notebook_time / 60:.2f} minutes\")\n    print(f\"  (Total Budget: {CONFIG.total_time_budget / 60:.2f} minutes)\")\n    print(f\"\\n  Final File: {OUTPUT_PATH}\")\n    if 'file_size' in locals():\n        print(f\"  Final Size: {file_size/1024:.1f} KB\")\n    \n    # --- HOTFIX 9: Close the logger ---\n    # *** MOVED INSIDE THE ELSE BLOCK ***\n    if 'controller' in locals() and hasattr(controller, 'metric_logger'):\n        controller.metric_logger.close()\n    \n    print(\"\\n  Ready for Kaggle submission.\")\n    print(\"=\"*70)\n#Cell 10\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T12:41:38.336269Z","iopub.execute_input":"2025-11-09T12:41:38.336573Z","iopub.status.idle":"2025-11-09T12:41:38.376117Z","shell.execute_reply.started":"2025-11-09T12:41:38.336548Z","shell.execute_reply":"2025-11-09T12:41:38.374855Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüåä‚öõÔ∏è LucidOrca Solver: Cell 10 (v2.0) - Main Inference Loop\n‚ùå CRITICAL ERROR: `controller` or `test_tasks` not found.\n   Please re-run Cell 8 and Cell 9 before this one.\n","output_type":"stream"}],"execution_count":10}]}