{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubtleGenius v1 - ARC Prize 2025 Submission System\n",
    "## Lean Agile Solo Vibe: Coder-LLM Collaborative AI Fusion\n",
    "\n",
    "**Architecture**: 6-Cell Modular Design\n",
    "**Philosophy**: Production-First, Token-Efficient, Never-Crash\n",
    "**Target**: Valid submission.json on first run, iterate to 85%+ accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## Cell Architecture\n",
    "1. **Configuration & Imports** - Zero-dependency foundation\n",
    "2. **Submission Validator** - Build BEFORE solving (Validation > Innovation)\n",
    "3. **Safe Defaults & Fallbacks** - Never crash (Valid 5% > Crashing 95%)\n",
    "4. **Submission Generator** - Wire to solver with production orchestration\n",
    "5. **Solver Logic** - Your AGI genius goes here (expand iteratively)\n",
    "6. **Execution Pipeline** - Load â†’ Solve â†’ Validate â†’ Save\n",
    "\n",
    "**Based on**: 48 hours of Ryan & Claude breakthrough insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 1: CONFIGURATION & IMPORTS\n",
    "# Zero cross-cell dependency issues - Single source of truth\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "print(\"ğŸ‹ SubtleGenius v1 - ARC Prize 2025\")\n",
    "print(f\"â° Initialized: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION (Meta-Optimized from 48hr Insights)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ARC2025Config:\n",
    "    \"\"\"Single source of truth for all configuration\"\"\"\n",
    "    \n",
    "    # Environment detection (auto-detect Kaggle vs local)\n",
    "    IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "    \n",
    "    if IS_KAGGLE:\n",
    "        # Kaggle paths\n",
    "        INPUT_PATH = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "        TRAIN_PATH = '/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json'\n",
    "        OUTPUT_PATH = '/kaggle/working/submission.json'\n",
    "        LOG_PATH = '/kaggle/working/solver_log.jsonl'\n",
    "    else:\n",
    "        # Local development paths\n",
    "        INPUT_PATH = 'data/arc-agi_test_challenges.json'\n",
    "        TRAIN_PATH = 'data/arc-agi_training_challenges.json'\n",
    "        OUTPUT_PATH = 'submission.json'\n",
    "        LOG_PATH = 'solver_log.jsonl'\n",
    "    \n",
    "    # Time budget management (95% rule - never terminate early!)\n",
    "    TOTAL_TIME_LIMIT = 12 * 3600  # 12 hours for ARC 2025\n",
    "    SAFETY_BUFFER = 0.05  # Use 95% of time (Insight #4)\n",
    "    EFFECTIVE_TIME = TOTAL_TIME_LIMIT * (1 - SAFETY_BUFFER)\n",
    "    \n",
    "    # Submission requirements (from official ARC Prize 2025 docs)\n",
    "    REQUIRED_ATTEMPTS = 2  # attempt_1 and attempt_2\n",
    "    GRID_VALUE_RANGE = (0, 9)  # Valid pixel values\n",
    "    \n",
    "    # Production settings\n",
    "    ENABLE_LOGGING = True\n",
    "    ENABLE_VALIDATION = True\n",
    "    ENABLE_FALLBACKS = True\n",
    "    VERBOSE = True\n",
    "    \n",
    "    # Dynamic difficulty multipliers (Insight #4: Dynamic Time Budgeting)\n",
    "    DIFFICULTY_MULTIPLIERS = {\n",
    "        'easy': 0.5,\n",
    "        'medium': 1.0,\n",
    "        'hard': 2.0,\n",
    "        'elite': 3.0\n",
    "    }\n",
    "\n",
    "config = ARC2025Config()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GLOBAL TIMER (95% Rule Enforcement)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class GlobalTimer:\n",
    "    \"\"\"Track time budget and enforce 95% rule\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def elapsed(self):\n",
    "        \"\"\"Time elapsed in seconds\"\"\"\n",
    "        return time.time() - self.start_time\n",
    "    \n",
    "    def remaining(self):\n",
    "        \"\"\"Time remaining in seconds\"\"\"\n",
    "        return config.EFFECTIVE_TIME - self.elapsed()\n",
    "    \n",
    "    def should_stop(self):\n",
    "        \"\"\"Check if we should wrap up (10min buffer)\"\"\"\n",
    "        return self.remaining() < 600\n",
    "    \n",
    "    def progress_pct(self):\n",
    "        \"\"\"Percentage of time budget used\"\"\"\n",
    "        return (self.elapsed() / config.EFFECTIVE_TIME) * 100\n",
    "\n",
    "timer = GlobalTimer()\n",
    "\n",
    "print(f\"âœ… Configuration loaded\")\n",
    "print(f\"   Environment: {'Kaggle' if config.IS_KAGGLE else 'Local'}\")\n",
    "print(f\"   Time budget: {config.EFFECTIVE_TIME/3600:.1f} hours\")\n",
    "print(f\"   Output: {config.OUTPUT_PATH}\")\n",
    "print(f\"   95% rule: {config.SAFETY_BUFFER*100}% safety buffer\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 2: SUBMISSION VALIDATOR\n",
    "# \"Validation > Innovation\" - Build BEFORE solving code\n",
    "# Prevents wasting daily submissions on format errors\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SubmissionValidator:\n",
    "    \"\"\"\n",
    "    Production-grade validator for ARC Prize 2025 submissions.\n",
    "    \n",
    "    Based on lessons from failed submission:\n",
    "    - âŒ List instead of dict\n",
    "    - âŒ Missing attempt_1/attempt_2 keys\n",
    "    - âŒ task_id as internal key (should be top-level)\n",
    "    - âŒ Single prediction (need 2 attempts)\n",
    "    - âŒ Not handling multiple test outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_grid(grid: List[List[int]], context: str = \"\") -> Tuple[bool, str]:\n",
    "        \"\"\"Validate a single grid structure\"\"\"\n",
    "        \n",
    "        # Must be a list\n",
    "        if not isinstance(grid, list):\n",
    "            return False, f\"{context}: Grid must be list, got {type(grid)}\"\n",
    "        \n",
    "        # Cannot be empty\n",
    "        if len(grid) == 0:\n",
    "            return False, f\"{context}: Grid cannot be empty\"\n",
    "        \n",
    "        # Must be 2D list\n",
    "        if not all(isinstance(row, list) for row in grid):\n",
    "            return False, f\"{context}: Grid must be 2D list\"\n",
    "        \n",
    "        # No ragged arrays (all rows same length)\n",
    "        if len(grid) > 0:\n",
    "            row_len = len(grid[0])\n",
    "            if not all(len(row) == row_len for row in grid):\n",
    "                return False, f\"{context}: Ragged array detected\"\n",
    "        \n",
    "        # All values must be integers 0-9\n",
    "        for i, row in enumerate(grid):\n",
    "            for j, val in enumerate(row):\n",
    "                if not isinstance(val, int):\n",
    "                    return False, f\"{context}[{i}][{j}]: Must be int, got {type(val)}\"\n",
    "                if val < config.GRID_VALUE_RANGE[0] or val > config.GRID_VALUE_RANGE[1]:\n",
    "                    return False, f\"{context}[{i}][{j}]: Value {val} not in range 0-9\"\n",
    "        \n",
    "        return True, \"Valid grid\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_prediction(prediction: Dict, context: str = \"\") -> Tuple[bool, str]:\n",
    "        \"\"\"Validate a single prediction dict (one test output)\"\"\"\n",
    "        \n",
    "        # Must be a dictionary\n",
    "        if not isinstance(prediction, dict):\n",
    "            return False, f\"{context}: Prediction must be dict, got {type(prediction)}\"\n",
    "        \n",
    "        # Must have both attempts\n",
    "        if \"attempt_1\" not in prediction:\n",
    "            return False, f\"{context}: Missing 'attempt_1' key\"\n",
    "        if \"attempt_2\" not in prediction:\n",
    "            return False, f\"{context}: Missing 'attempt_2' key\"\n",
    "        \n",
    "        # Validate both attempt grids\n",
    "        for attempt_key in [\"attempt_1\", \"attempt_2\"]:\n",
    "            grid = prediction[attempt_key]\n",
    "            is_valid, msg = SubmissionValidator.validate_grid(\n",
    "                grid, \n",
    "                context=f\"{context}[{attempt_key}]\"\n",
    "            )\n",
    "            if not is_valid:\n",
    "                return False, msg\n",
    "        \n",
    "        return True, \"Valid prediction\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_task_predictions(task_id: str, \n",
    "                                  predictions: List[Dict],\n",
    "                                  expected_count: int) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate all predictions for a single task\"\"\"\n",
    "        \n",
    "        # Must be a list\n",
    "        if not isinstance(predictions, list):\n",
    "            return False, f\"Task {task_id}: predictions must be list, got {type(predictions)}\"\n",
    "        \n",
    "        # Must match expected number of test outputs\n",
    "        if len(predictions) != expected_count:\n",
    "            return False, f\"Task {task_id}: expected {expected_count} predictions, got {len(predictions)}\"\n",
    "        \n",
    "        # Validate each prediction\n",
    "        for idx, prediction in enumerate(predictions):\n",
    "            is_valid, msg = SubmissionValidator.validate_prediction(\n",
    "                prediction,\n",
    "                context=f\"Task {task_id}[{idx}]\"\n",
    "            )\n",
    "            if not is_valid:\n",
    "                return False, msg\n",
    "        \n",
    "        return True, \"Valid task predictions\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_submission(submission: Dict, \n",
    "                           test_challenges: Dict) -> Tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validate complete submission.json structure.\n",
    "        This is the MASTER validation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ” VALIDATING SUBMISSION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # 1. Must be a dictionary (NOT a list!)\n",
    "        if not isinstance(submission, dict):\n",
    "            return False, f\"âŒ Submission must be DICT, got {type(submission)}\"\n",
    "        \n",
    "        print(f\"âœ… Structure: Dictionary (not list)\")\n",
    "        \n",
    "        # 2. Must have ALL task IDs from test set\n",
    "        test_task_ids = set(test_challenges.keys())\n",
    "        submission_task_ids = set(submission.keys())\n",
    "        \n",
    "        missing_tasks = test_task_ids - submission_task_ids\n",
    "        extra_tasks = submission_task_ids - test_task_ids\n",
    "        \n",
    "        if missing_tasks:\n",
    "            return False, f\"âŒ Missing tasks: {list(missing_tasks)[:5]}\"\n",
    "        if extra_tasks:\n",
    "            return False, f\"âŒ Extra tasks: {list(extra_tasks)[:5]}\"\n",
    "        \n",
    "        print(f\"âœ… Task coverage: {len(submission_task_ids)} tasks (complete)\")\n",
    "        \n",
    "        # 3. Validate each task\n",
    "        errors = []\n",
    "        for task_id, predictions in submission.items():\n",
    "            expected_count = len(test_challenges[task_id]['test'])\n",
    "            is_valid, msg = SubmissionValidator.validate_task_predictions(\n",
    "                task_id, predictions, expected_count\n",
    "            )\n",
    "            if not is_valid:\n",
    "                errors.append(msg)\n",
    "                if len(errors) >= 5:  # Only show first 5 errors\n",
    "                    break\n",
    "        \n",
    "        if errors:\n",
    "            return False, f\"âŒ Validation errors:\\n\" + \"\\n\".join(errors)\n",
    "        \n",
    "        print(f\"âœ… All predictions: Valid format\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ‰ SUBMISSION VALIDATION PASSED!\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return True, \"Valid submission\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_and_save(submission: Dict, \n",
    "                         test_challenges: Dict,\n",
    "                         output_path: str = None) -> bool:\n",
    "        \"\"\"Validate and save submission in one atomic operation\"\"\"\n",
    "        \n",
    "        if output_path is None:\n",
    "            output_path = config.OUTPUT_PATH\n",
    "        \n",
    "        # Validate first\n",
    "        is_valid, msg = SubmissionValidator.validate_submission(submission, test_challenges)\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(f\"âŒ VALIDATION FAILED: {msg}\")\n",
    "            print(\"âŒ Submission NOT saved\")\n",
    "            return False\n",
    "        \n",
    "        # Save\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(submission, f, indent=2)\n",
    "            \n",
    "            file_size = os.path.getsize(output_path)\n",
    "            print(f\"âœ… Saved to: {output_path}\")\n",
    "            print(f\"   File size: {file_size:,} bytes\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"âœ… Validator loaded\")\n",
    "print(\"   Catches: dict vs list, missing attempts, ragged arrays, invalid values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 3: SAFE DEFAULTS & FALLBACKS\n",
    "# \"Valid 5% > Crashing 95%\" - Never crash, always return valid\n",
    "# Production-grade error recovery (Insight #6)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SafeDefaults:\n",
    "    \"\"\"\n",
    "    Production-grade fallback strategies.\n",
    "    These NEVER throw exceptions. They ALWAYS return valid grids.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def copy_input(task_data: Dict, test_idx: int = 0) -> List[List[int]]:\n",
    "        \"\"\"Strategy 1: Copy test input (safest default)\"\"\"\n",
    "        try:\n",
    "            return task_data['test'][test_idx]['input']\n",
    "        except:\n",
    "            return [[0]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def copy_train_output(task_data: Dict, train_idx: int = 0) -> List[List[int]]:\n",
    "        \"\"\"Strategy 2: Copy first training output\"\"\"\n",
    "        try:\n",
    "            return task_data['train'][train_idx]['output']\n",
    "        except:\n",
    "            return SafeDefaults.copy_input(task_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def blank_grid(task_data: Dict, test_idx: int = 0) -> List[List[int]]:\n",
    "        \"\"\"Strategy 3: Generate blank grid matching input size\"\"\"\n",
    "        try:\n",
    "            test_input = task_data['test'][test_idx]['input']\n",
    "            rows = len(test_input)\n",
    "            cols = len(test_input[0]) if rows > 0 else 1\n",
    "            return [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "        except:\n",
    "            return [[0]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity_transform(task_data: Dict, test_idx: int = 0) -> List[List[int]]:\n",
    "        \"\"\"Strategy 4: Identity (copy input)\"\"\"\n",
    "        return SafeDefaults.copy_input(task_data, test_idx)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_two_attempts(task_data: Dict, test_idx: int = 0) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "        \"\"\"\n",
    "        Get two safe default attempts.\n",
    "        Attempt 1: Copy input\n",
    "        Attempt 2: Try train output, fallback to blank\n",
    "        \"\"\"\n",
    "        attempt_1 = SafeDefaults.copy_input(task_data, test_idx)\n",
    "        \n",
    "        try:\n",
    "            attempt_2 = SafeDefaults.copy_train_output(task_data)\n",
    "        except:\n",
    "            attempt_2 = SafeDefaults.blank_grid(task_data, test_idx)\n",
    "        \n",
    "        return attempt_1, attempt_2\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_execute(func, *args, fallback_func=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Execute function with automatic fallback.\n",
    "        NEVER crashes. ALWAYS returns something valid.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Validate result is a grid\n",
    "            is_valid, _ = SubmissionValidator.validate_grid(result)\n",
    "            if is_valid:\n",
    "                return result\n",
    "            else:\n",
    "                if config.VERBOSE:\n",
    "                    print(f\"âš ï¸  {func.__name__} returned invalid grid, using fallback\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            if config.VERBOSE:\n",
    "                print(f\"âš ï¸  {func.__name__} failed: {str(e)[:50]}\")\n",
    "        \n",
    "        # Use fallback\n",
    "        if fallback_func:\n",
    "            try:\n",
    "                return fallback_func(*args, **kwargs)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Ultimate fallback\n",
    "        if 'task_data' in kwargs:\n",
    "            return SafeDefaults.copy_input(kwargs['task_data'])\n",
    "        elif len(args) > 1 and isinstance(args[1], dict):\n",
    "            return SafeDefaults.copy_input(args[1])\n",
    "        else:\n",
    "            return [[0]]\n",
    "\n",
    "print(\"âœ… Safe defaults loaded\")\n",
    "print(\"   4-tier fallback: copy_input â†’ copy_train â†’ blank_grid â†’ [[0]]\")\n",
    "print(\"   Never crashes. Always returns valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 4: SUBMISSION GENERATOR\n",
    "# Wire to solver with production-grade orchestration\n",
    "# Enforces correct format: dict, not list! (Insight from failed submission)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SubmissionGenerator:\n",
    "    \"\"\"\n",
    "    Generate perfectly formatted submission.json for ARC Prize 2025.\n",
    "    \n",
    "    Correct format (learned from failure):\n",
    "    {\n",
    "      \"task_id\": [\n",
    "        {\"attempt_1\": [[...]], \"attempt_2\": [[...]]},\n",
    "        # More dicts if multiple test outputs\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, solver_func=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            solver_func: Function(test_input, task_data, attempt) -> grid\n",
    "            verbose: Enable progress logging\n",
    "        \"\"\"\n",
    "        self.solver_func = solver_func\n",
    "        self.verbose = verbose\n",
    "        self.stats = {\n",
    "            'total_tasks': 0,\n",
    "            'total_predictions': 0,\n",
    "            'solver_successes': 0,\n",
    "            'fallback_uses': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def generate_attempts(self, \n",
    "                         test_input: List[List[int]], \n",
    "                         task_data: Dict,\n",
    "                         test_idx: int = 0) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "        \"\"\"\n",
    "        Generate two attempts for a single test input.\n",
    "        Uses solver if available, falls back to safe defaults.\n",
    "        \"\"\"\n",
    "        \n",
    "        attempt_1 = None\n",
    "        attempt_2 = None\n",
    "        \n",
    "        # Try solver for attempt 1\n",
    "        if self.solver_func:\n",
    "            attempt_1 = SafeDefaults.safe_execute(\n",
    "                self.solver_func,\n",
    "                test_input=test_input,\n",
    "                task_data=task_data,\n",
    "                attempt=1,\n",
    "                fallback_func=None\n",
    "            )\n",
    "            \n",
    "            if attempt_1 is not None and isinstance(attempt_1, list):\n",
    "                self.stats['solver_successes'] += 1\n",
    "        \n",
    "        # Fallback for attempt 1\n",
    "        if attempt_1 is None:\n",
    "            attempt_1 = SafeDefaults.copy_input(task_data, test_idx)\n",
    "            self.stats['fallback_uses'] += 1\n",
    "        \n",
    "        # Try solver for attempt 2 (with variation)\n",
    "        if self.solver_func:\n",
    "            attempt_2 = SafeDefaults.safe_execute(\n",
    "                self.solver_func,\n",
    "                test_input=test_input,\n",
    "                task_data=task_data,\n",
    "                attempt=2,\n",
    "                fallback_func=None\n",
    "            )\n",
    "            \n",
    "            if attempt_2 is not None and isinstance(attempt_2, list):\n",
    "                self.stats['solver_successes'] += 1\n",
    "        \n",
    "        # Fallback for attempt 2 (different strategy)\n",
    "        if attempt_2 is None:\n",
    "            attempt_2 = SafeDefaults.copy_train_output(task_data)\n",
    "            self.stats['fallback_uses'] += 1\n",
    "        \n",
    "        return attempt_1, attempt_2\n",
    "    \n",
    "    def generate_submission(self, \n",
    "                           test_challenges: Dict) -> Dict[str, List[Dict[str, List[List[int]]]]]:\n",
    "        \"\"\"\n",
    "        Generate complete submission from test challenges.\n",
    "        \n",
    "        Returns:\n",
    "            Properly formatted submission dictionary ready for validation\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ”¨ GENERATING SUBMISSION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        submission = {}  # DICT, not list!\n",
    "        total_tasks = len(test_challenges)\n",
    "        \n",
    "        for task_idx, (task_id, task_data) in enumerate(test_challenges.items(), 1):\n",
    "            \n",
    "            # Progress update\n",
    "            if self.verbose and task_idx % 20 == 0:\n",
    "                pct = (task_idx / total_tasks) * 100\n",
    "                elapsed = timer.elapsed() / 60\n",
    "                print(f\"   [{task_idx:3}/{total_tasks}] {pct:5.1f}% | \"\n",
    "                      f\"Elapsed: {elapsed:.1f}min | \"\n",
    "                      f\"Solver: {self.stats['solver_successes']} | \"\n",
    "                      f\"Fallback: {self.stats['fallback_uses']}\")\n",
    "            \n",
    "            # Check time budget (95% rule)\n",
    "            if timer.should_stop():\n",
    "                print(f\"â° Approaching time limit, wrapping up...\")\n",
    "                # Fill remaining tasks with safe defaults\n",
    "                for remaining_id in list(test_challenges.keys())[task_idx-1:]:\n",
    "                    remaining_data = test_challenges[remaining_id]\n",
    "                    num_tests = len(remaining_data['test'])\n",
    "                    submission[remaining_id] = [\n",
    "                        {\n",
    "                            \"attempt_1\": SafeDefaults.copy_input(remaining_data, i),\n",
    "                            \"attempt_2\": SafeDefaults.copy_train_output(remaining_data)\n",
    "                        }\n",
    "                        for i in range(num_tests)\n",
    "                    ]\n",
    "                break\n",
    "            \n",
    "            # Generate predictions for this task\n",
    "            task_predictions = []\n",
    "            num_test_outputs = len(task_data['test'])\n",
    "            \n",
    "            for test_idx in range(num_test_outputs):\n",
    "                try:\n",
    "                    test_input = task_data['test'][test_idx]['input']\n",
    "                    \n",
    "                    # Generate two attempts\n",
    "                    attempt_1, attempt_2 = self.generate_attempts(\n",
    "                        test_input, task_data, test_idx\n",
    "                    )\n",
    "                    \n",
    "                    # Create prediction dict\n",
    "                    prediction = {\n",
    "                        \"attempt_1\": attempt_1,\n",
    "                        \"attempt_2\": attempt_2\n",
    "                    }\n",
    "                    \n",
    "                    # Validate before adding\n",
    "                    is_valid, msg = SubmissionValidator.validate_prediction(\n",
    "                        prediction,\n",
    "                        context=f\"Task {task_id}[{test_idx}]\"\n",
    "                    )\n",
    "                    \n",
    "                    if not is_valid:\n",
    "                        print(f\"âš ï¸  {msg}, using safe defaults\")\n",
    "                        attempt_1, attempt_2 = SafeDefaults.get_two_attempts(task_data, test_idx)\n",
    "                        prediction = {\"attempt_1\": attempt_1, \"attempt_2\": attempt_2}\n",
    "                    \n",
    "                    task_predictions.append(prediction)\n",
    "                    self.stats['total_predictions'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error on task {task_id}[{test_idx}]: {e}\")\n",
    "                    self.stats['errors'].append({'task_id': task_id, 'test_idx': test_idx, 'error': str(e)})\n",
    "                    \n",
    "                    # Emergency fallback\n",
    "                    attempt_1, attempt_2 = SafeDefaults.get_two_attempts(task_data, test_idx)\n",
    "                    task_predictions.append({\"attempt_1\": attempt_1, \"attempt_2\": attempt_2})\n",
    "            \n",
    "            # Add to submission (task_id as TOP-LEVEL KEY)\n",
    "            submission[task_id] = task_predictions\n",
    "            self.stats['total_tasks'] += 1\n",
    "        \n",
    "        print(f\"\\nâœ… Generated {self.stats['total_tasks']} tasks, \"\n",
    "              f\"{self.stats['total_predictions']} predictions\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print generation statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š GENERATION STATISTICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total tasks: {self.stats['total_tasks']}\")\n",
    "        print(f\"Total predictions: {self.stats['total_predictions']}\")\n",
    "        print(f\"Solver successes: {self.stats['solver_successes']}\")\n",
    "        print(f\"Fallback uses: {self.stats['fallback_uses']}\")\n",
    "        print(f\"Errors: {len(self.stats['errors'])}\")\n",
    "        \n",
    "        if self.stats['errors'] and config.VERBOSE:\n",
    "            print(\"\\nError summary (first 5):\")\n",
    "            for err in self.stats['errors'][:5]:\n",
    "                print(f\"  - {err['task_id']}[{err['test_idx']}]: {err['error'][:60]}\")\n",
    "        \n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"âœ… Submission generator loaded\")\n",
    "print(\"   Format: {task_id: [{attempt_1: ..., attempt_2: ...}]}\")\n",
    "print(\"   Enforces: DICT structure, both attempts, multiple test outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 5: SOLVER LOGIC\n",
    "# Your AGI genius goes here - expand iteratively (token-efficient)\n",
    "# Start simple (200 lines), test end-to-end, THEN expand (800+ lines)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def simple_solver(test_input: List[List[int]], \n",
    "                  task_data: Dict,\n",
    "                  attempt: int = 1) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Placeholder solver - REPLACE with your actual solving logic.\n",
    "    \n",
    "    Args:\n",
    "        test_input: The test input grid to solve\n",
    "        task_data: Complete task data (includes train examples)\n",
    "        attempt: Which attempt (1 or 2) - can use different strategies\n",
    "    \n",
    "    Returns:\n",
    "        Predicted output grid (2D list of integers 0-9)\n",
    "    \n",
    "    TODO: Expand with insights from 48hr collaboration:\n",
    "    - Phase 1: Pattern matching (flip, rotate, color swap)\n",
    "    - Phase 2: Object detection (connected components)\n",
    "    - Phase 3: Ensemble methods (tank/dps/healer/pug specialists)\n",
    "    - Phase 4: Meta-cognition (lambda dictionary cognitive modes)\n",
    "    - Phase 5: Championship polish (85%+ accuracy)\n",
    "    \"\"\"\n",
    "    \n",
    "    # For now: simple identity transform (baseline)\n",
    "    # This ensures end-to-end pipeline works\n",
    "    return test_input\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# EXTENSION POINTS (expand as you iterate)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# def pattern_matcher_solver(test_input, task_data, attempt):\n",
    "#     \"\"\"Phase 1: Detect simple transformations\"\"\"\n",
    "#     # TODO: Implement flip, rotate, color swap\n",
    "#     pass\n",
    "\n",
    "# def object_detection_solver(test_input, task_data, attempt):\n",
    "#     \"\"\"Phase 2: Object-based reasoning\"\"\"\n",
    "#     # TODO: Connected components, bounding boxes\n",
    "#     pass\n",
    "\n",
    "# def ensemble_solver(test_input, task_data, attempt):\n",
    "#     \"\"\"Phase 3: Coordinate multiple specialists (Insight #7)\"\"\"\n",
    "#     # TODO: Tank/DPS/Healer/PUG ensemble\n",
    "#     pass\n",
    "\n",
    "# def meta_cognitive_solver(test_input, task_data, attempt):\n",
    "#     \"\"\"Phase 4: Self-reflective reasoning (Insight #8)\"\"\"\n",
    "#     # TODO: Lambda dictionary cognitive modes\n",
    "#     pass\n",
    "\n",
    "print(\"âœ… Solver logic loaded (placeholder - expand with your AGI)\")\n",
    "print(\"   Current: Identity transform (baseline)\")\n",
    "print(\"   Expand to: Pattern matching â†’ Objects â†’ Ensemble â†’ Meta-cognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CELL 6: EXECUTION PIPELINE\n",
    "# Orchestration: Load â†’ Solve â†’ Generate â†’ Validate â†’ Save\n",
    "# Implements 95% rule, progress tracking, production monitoring\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline.\n",
    "    Follows production-first, 95% rule principles.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‹ SUBTLEGENIUS V1 - ARC PRIZE 2025 SUBMISSION SYSTEM\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"â° Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(f\"ğŸ’¾ Output: {config.OUTPUT_PATH}\")\n",
    "    print(f\"ğŸ• Time budget: {config.EFFECTIVE_TIME/3600:.1f}h (95% of 12h)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # STEP 1: Load test data\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    print(\"ğŸ“‚ Loading test challenges...\")\n",
    "    try:\n",
    "        with open(config.INPUT_PATH, 'r') as f:\n",
    "            test_challenges = json.load(f)\n",
    "        print(f\"âœ… Loaded {len(test_challenges)} tasks\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load test data: {e}\")\n",
    "        print(f\"   Path: {config.INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # STEP 2: Generate submission\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    generator = SubmissionGenerator(\n",
    "        solver_func=simple_solver,  # REPLACE with your solver\n",
    "        verbose=config.VERBOSE\n",
    "    )\n",
    "    \n",
    "    submission = generator.generate_submission(test_challenges)\n",
    "    generator.print_stats()\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # STEP 3: Validate and save submission\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    if config.ENABLE_VALIDATION:\n",
    "        is_valid = SubmissionValidator.validate_and_save(\n",
    "            submission,\n",
    "            test_challenges,\n",
    "            config.OUTPUT_PATH\n",
    "        )\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"âŒ CRITICAL: Submission validation failed!\")\n",
    "            print(\"âŒ Fix errors before submitting to Kaggle!\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            return\n",
    "    else:\n",
    "        # Save without validation (not recommended)\n",
    "        with open(config.OUTPUT_PATH, 'w') as f:\n",
    "            json.dump(submission, f, indent=2)\n",
    "        print(f\"âœ… Saved (without validation): {config.OUTPUT_PATH}\")\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # STEP 4: Final summary\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    elapsed = timer.elapsed()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"â±ï¸  Total time: {elapsed:.1f}s ({elapsed/60:.1f}min)\")\n",
    "    print(f\"ğŸ“Š Time used: {timer.progress_pct():.1f}% of budget\")\n",
    "    print(f\"ğŸ“ Submission: {config.OUTPUT_PATH}\")\n",
    "    \n",
    "    if os.path.exists(config.OUTPUT_PATH):\n",
    "        file_size = os.path.getsize(config.OUTPUT_PATH)\n",
    "        print(f\"ğŸ“ File size: {file_size:,} bytes\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nğŸš€ Ready for Kaggle submission!\")\n",
    "    print(\"   Remember:\")\n",
    "    print(\"   1. Disable internet access\")\n",
    "    print(\"   2. Open source before getting official scores\")\n",
    "    print(\"   3. Save logs for post-submission analysis\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Cleanup\n",
    "    gc.collect()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RUN THE PIPELINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"âœ… Execution pipeline ready\")\n",
    "print(\"   Run main() to generate submission.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
