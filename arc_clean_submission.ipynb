{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARC Prize 2025 - Clean Solver Submission\n",
        "\n",
        "**Approach:** Practical ensemble solver with task classification  \n",
        "**Target:** 15-25% accuracy (realistic baseline)  \n",
        "**Author:** Ryan Cardwell & Claude  \n",
        "**Date:** November 2025\n",
        "\n",
        "## Strategy:\n",
        "1. **Task Classification** - Route tasks to specialized solvers\n",
        "2. **Ensemble Voting** - Multiple solvers vote, agreement = confidence  \n",
        "3. **Dual Attempts** - Submit 2 solutions per task\n",
        "4. **Time Budget Management** - Allocate time efficiently\n",
        "5. **Robust Fallbacks** - Always submit something"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "print(\"Environment check:\")\n",
        "print(f\"  Python: {sys.version.split()[0]}\")\n",
        "print(f\"  NumPy: {np.__version__}\")\n",
        "print(f\"  Working dir: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Copy Solver File\n",
        "\n",
        "If using Kaggle dataset, copy the solver file. Otherwise it should be in the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Try to copy from Kaggle dataset\n",
        "solver_file = 'arc_clean_solver.py'\n",
        "kaggle_path = Path(f'/kaggle/input/arc-solver-clean/{solver_file}')\n",
        "working_path = Path(f'/kaggle/working/{solver_file}')\n",
        "\n",
        "if kaggle_path.exists():\n",
        "    shutil.copy(kaggle_path, working_path)\n",
        "    print(f\"Copied {solver_file} from dataset\")\n",
        "elif Path(solver_file).exists():\n",
        "    print(f\"{solver_file} already in working directory\")\n",
        "else:\n",
        "    print(f\"WARNING: {solver_file} not found!\")\n",
        "    print(\"Will try to import anyway (may be inline)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add working directory to path\n",
        "sys.path.insert(0, '/kaggle/working')\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "try:\n",
        "    from arc_clean_solver import ARCCleanSolver, SolverConfig, save_submission\n",
        "    print(\"Successfully imported ARC Clean Solver\")\n",
        "except ImportError as e:\n",
        "    print(f\"ERROR: Could not import solver: {e}\")\n",
        "    print(\"\\nYou need to either:\")\n",
        "    print(\"  1. Upload arc_clean_solver.py as a Kaggle dataset\")\n",
        "    print(\"  2. Include it in this notebook's files\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ARC PRIZE 2025 - CLEAN SOLVER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Determine data path\n",
        "data_paths = [\n",
        "    Path('/kaggle/input/arc-prize-2025'),\n",
        "    Path('.'),\n",
        "]\n",
        "\n",
        "data_path = None\n",
        "for p in data_paths:\n",
        "    if (p / 'arc-agi_test_challenges.json').exists():\n",
        "        data_path = p\n",
        "        break\n",
        "\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError(\"Could not find ARC test data!\")\n",
        "\n",
        "# Load test tasks\n",
        "test_path = data_path / 'arc-agi_test_challenges.json'\n",
        "print(f\"\\nLoading: {test_path}\")\n",
        "\n",
        "with open(test_path, 'r') as f:\n",
        "    test_tasks = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_tasks)} test tasks\")\n",
        "print(f\"\\nSample tasks:\")\n",
        "for i, (task_id, task) in enumerate(list(test_tasks.items())[:3]):\n",
        "    print(f\"  {i+1}. {task_id}: {len(task['train'])} train, {len(task['test'])} test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure solver\n",
        "config = SolverConfig(\n",
        "    total_time_budget=6 * 3600,  # 6 hours (leave buffer for 9hr limit)\n",
        "    min_time_per_task=0.5,\n",
        "    max_time_per_task=30.0,\n",
        "    enable_task_classification=True,\n",
        "    enable_ensemble_voting=True,\n",
        "    output_path='/kaggle/working'\n",
        ")\n",
        "\n",
        "print(\"\\nInitializing solver...\")\n",
        "solver = ARCCleanSolver(config)\n",
        "print(\"Solver ready!\")\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Time budget: {config.total_time_budget/3600:.1f} hours\")\n",
        "print(f\"  Time per task: {config.min_time_per_task}s - {config.max_time_per_task}s\")\n",
        "print(f\"  Task classification: {config.enable_task_classification}\")\n",
        "print(f\"  Ensemble voting: {config.enable_ensemble_voting}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solve Test Set\n",
        "\n",
        "This will take approximately 3-6 hours depending on task complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING SOLVE PROCESS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    submission = solver.solve_test_set(test_tasks)\n",
        "    solve_success = True\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR during solving: {e}\")\n",
        "    print(\"Creating fallback submission...\")\n",
        "    \n",
        "    # Emergency fallback\n",
        "    submission = {}\n",
        "    for task_id, task in test_tasks.items():\n",
        "        test_input = np.array(task['test'][0]['input'])\n",
        "        num_outputs = len(task['test'])\n",
        "        submission[task_id] = [{\n",
        "            'attempt_1': test_input.tolist(),\n",
        "            'attempt_2': np.rot90(test_input).tolist()\n",
        "        } for _ in range(num_outputs)]\n",
        "    \n",
        "    solve_success = False\n",
        "\n",
        "solve_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nSolve time: {solve_time:.0f}s ({solve_time/60:.1f} min)\")\n",
        "print(f\"Tasks in submission: {len(submission)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_submission(submission: dict, expected_tasks: dict) -> bool:\n",
        "    \"\"\"Validate submission format\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"VALIDATING SUBMISSION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    errors = []\n",
        "    \n",
        "    # Check task count\n",
        "    if len(submission) != len(expected_tasks):\n",
        "        errors.append(f\"Task count mismatch: {len(submission)} vs {len(expected_tasks)}\")\n",
        "    \n",
        "    # Check each task\n",
        "    for task_id, task in expected_tasks.items():\n",
        "        if task_id not in submission:\n",
        "            errors.append(f\"Missing task: {task_id}\")\n",
        "            continue\n",
        "        \n",
        "        task_solution = submission[task_id]\n",
        "        \n",
        "        # Check format\n",
        "        if not isinstance(task_solution, list):\n",
        "            errors.append(f\"{task_id}: Not a list\")\n",
        "            continue\n",
        "        \n",
        "        # Check number of test outputs\n",
        "        expected_outputs = len(task['test'])\n",
        "        if len(task_solution) != expected_outputs:\n",
        "            errors.append(f\"{task_id}: Wrong number of outputs ({len(task_solution)} vs {expected_outputs})\")\n",
        "        \n",
        "        # Check each output has attempt_1 and attempt_2\n",
        "        for i, output in enumerate(task_solution):\n",
        "            if not isinstance(output, dict):\n",
        "                errors.append(f\"{task_id}[{i}]: Not a dict\")\n",
        "                continue\n",
        "            \n",
        "            if 'attempt_1' not in output or 'attempt_2' not in output:\n",
        "                errors.append(f\"{task_id}[{i}]: Missing attempt_1 or attempt_2\")\n",
        "    \n",
        "    # Print results\n",
        "    if errors:\n",
        "        print(f\"\\nFOUND {len(errors)} ERRORS:\")\n",
        "        for error in errors[:10]:  # Show first 10\n",
        "            print(f\"  - {error}\")\n",
        "        if len(errors) > 10:\n",
        "            print(f\"  ... and {len(errors)-10} more\")\n",
        "        print(\"\\nVALIDATION FAILED\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"\\nAll checks passed!\")\n",
        "        print(f\"  Tasks: {len(submission)}\")\n",
        "        print(f\"  Format: Valid\")\n",
        "        print(\"\\nVALIDATION PASSED\")\n",
        "        return True\n",
        "\n",
        "# Validate\n",
        "is_valid = validate_submission(submission, test_tasks)\n",
        "\n",
        "if not is_valid:\n",
        "    raise ValueError(\"Submission validation failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save using the built-in save function\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAVING SUBMISSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "clean_submission = save_submission(submission, config)\n",
        "\n",
        "# Verify file exists\n",
        "submission_path = Path('/kaggle/working/submission.json')\n",
        "if submission_path.exists():\n",
        "    file_size = submission_path.stat().st_size\n",
        "    print(f\"\\nSubmission file created successfully!\")\n",
        "    print(f\"  Path: {submission_path}\")\n",
        "    print(f\"  Size: {file_size/1024:.1f} KB\")\n",
        "    print(f\"  Tasks: {len(clean_submission)}\")\n",
        "else:\n",
        "    print(f\"\\nWARNING: Could not verify submission file at {submission_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUBMISSION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  Total time: {total_time:.0f}s ({total_time/60:.1f} min)\")\n",
        "print(f\"  Tasks solved: {len(submission)}\")\n",
        "print(f\"  Average time per task: {total_time/len(submission):.2f}s\")\n",
        "print(f\"  Solve success: {'Yes' if solve_success else 'No (used fallback)'}\")\n",
        "\n",
        "if solver.stats:\n",
        "    print(f\"\\nSolver Statistics:\")\n",
        "    print(f\"  High confidence: {solver.stats.get('high_confidence', 0)}\")\n",
        "    print(f\"  Fallbacks: {solver.stats.get('fallbacks', 0)}\")\n",
        "    \n",
        "    by_category = solver.stats.get('by_category', {})\n",
        "    if by_category:\n",
        "        print(f\"\\n  By category:\")\n",
        "        for cat, count in by_category.most_common():\n",
        "            print(f\"    {cat}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Ready for Kaggle submission!\")\n",
        "print(\"Expected accuracy: 15-25% (realistic baseline)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### What This Solver Does:\n",
        "1. **Classifies tasks** into categories (geometric, color, spatial, pattern, complex)\n",
        "2. **Routes to specialists** - Different solvers for different task types\n",
        "3. **Ensemble voting** - Multiple solvers agree = higher confidence\n",
        "4. **Dual attempts** - Submits 2 different solutions per task\n",
        "5. **Time management** - Allocates time efficiently across all tasks\n",
        "6. **Robust fallbacks** - Always submits something, never fails\n",
        "\n",
        "### Why 15-25% is Realistic:\n",
        "- Baseline (random): ~4%\n",
        "- Simple pattern matching: ~10-15%\n",
        "- This solver (multi-strategy): ~15-25%\n",
        "- SOTA (complex neural): ~35-45%\n",
        "- Human performance: ~80-90%\n",
        "\n",
        "### Future Improvements:\n",
        "1. Add more specialized solvers (object tracking, counting, etc.)\n",
        "2. Implement learned features from training data\n",
        "3. Add symbolic reasoning for complex tasks\n",
        "4. Improve variation generation for attempt_2\n",
        "5. Meta-learning across tasks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
