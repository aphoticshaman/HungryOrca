# Training Divergence - Root Cause & Fix

## üö® The Problem You Reported

```
Epoch 0: Loss = 0.6000
Epoch 5: Loss = 0.7400  ‚Üê GETTING WORSE!
```

**This is called "training divergence"** - the model is getting worse instead of better.

---

## üîç Root Cause Analysis

### **Bug #1: Warmup + Scheduler Conflict** (CRITICAL)

**What train_ULTIMATE.py v1 did (WRONG):**

```python
# Training loop (EVERY STEP):
def get_lr_scale(step, warmup_steps=500):
    if step < warmup_steps:
        return step / warmup_steps
    return 1.0

# Inside training loop:
lr_scale = get_lr_scale(global_step, 500)
for param_group in optimizer.param_groups:
    param_group['lr'] = CONFIG['learning_rate'] * lr_scale  # ‚Üê Set LR

# End of epoch:
scheduler.step()  # ‚Üê Scheduler ALSO modifies LR
```

**What happened:**

```
Epoch 0 (steps 0-2842):
  Step 0:    LR = 1e-4 √ó 0.001 = 1e-7 (warmup start)
  Step 250:  LR = 1e-4 √ó 0.5   = 5e-5 (halfway warmup)
  Step 500:  LR = 1e-4 √ó 1.0   = 1e-4 (warmup done)
  Step 2842: LR = 1e-4 √ó 1.0   = 1e-4
  Epoch end: scheduler.step() ‚Üí LR = 9.9e-5 ‚úì

Epoch 1 (steps 2843-5684):
  Step 2843: global_step = 2843 > 500, so lr_scale = 1.0
             LR = 1e-4 √ó 1.0 = 1e-4  ‚Üê OVERWRITES scheduler!

  Scheduler set LR to 9.9e-5, but warmup code reset it to 1e-4!
  Scheduler is being ignored!
```

**Even worse - the real bug:**

Actually, looking at the code more carefully, `global_step` resets might not be happening, but the REAL issue is:

```python
# global_step keeps increasing: 0, 1, 2, ..., 5684, 5685...
# Warmup is ONLY for first 500 steps
# But we're modifying LR EVERY STEP even after warmup!

for param_group in optimizer.param_groups:
    param_group['lr'] = CONFIG['learning_rate'] * lr_scale

# This OVERWRITES whatever the scheduler set!
# Scheduler tries to decay LR, warmup code sets it back to 1e-4
```

**Result**: Learning rate never decreases, stays at 3e-4, model diverges!

---

### **Bug #2: Learning Rate Too High**

```python
'learning_rate': 3e-4,  # Too aggressive
```

Your loss started at 0.6, which is actually pretty low for random initialization. Typical ARC loss starts at ~2.3 (log(10) for 10 classes).

**Starting at 0.6 suggests**:
- Your problem is easier than expected OR
- Model initialization is better than expected OR
- You need a gentler learning rate

With LR = 3e-4, the model was taking huge steps and overshooting the minimum.

```
Loss landscape (1D simplified):

     0.8  |    \              /
          |     \            /
     0.6  |      \    X     /    ‚Üê You start here
          |       \  /|\   /
     0.4  |        \/   \ /
          |         MIN ‚Üê  Want to reach here
     0.2  |
```

With LR too high:
```
Step 1: Loss 0.6 ‚Üí 0.65 (jumped too far, missed minimum)
Step 2: Loss 0.65 ‚Üí 0.70 (jumped back, still missed)
Step 3: Loss 0.70 ‚Üí 0.74 (oscillating, diverging)
```

---

### **Bug #3: Label Smoothing Too Early**

```python
loss = F.cross_entropy(..., label_smoothing=0.1)
```

**What label smoothing does**:
- Normal: Target is [0, 0, 1, 0, 0] (100% confidence)
- Smoothed: Target is [0.01, 0.01, 0.9, 0.01, 0.01] (90% confidence)

**Why it's a problem early**:
- Model hasn't learned basics yet
- Label smoothing says "be less confident"
- Model gets confused about what to learn
- Loss increases because targets are "fuzzy"

**When to use it**: After model converges to ~80% accuracy, THEN add smoothing for final 5-10% improvement.

---

## ‚úÖ The Fix (train_ULTIMATE_v2.py)

### **Fix #1: Proper Warmup** ‚úì

```python
def get_lr_multiplier(epoch, warmup_epochs):
    """Epoch-based warmup (not step-based)"""
    if epoch < warmup_epochs:
        return (epoch + 1) / warmup_epochs
    return 1.0

# In training loop:
warmup_mult = get_lr_multiplier(epoch, CONFIG['warmup_epochs'])
current_lr = scheduler.get_last_lr()[0] * warmup_mult  # ‚Üê Multiply scheduler's LR

for param_group in optimizer.param_groups:
    param_group['lr'] = current_lr

# After epoch:
scheduler.step()  # ‚Üê This works now!
```

**What happens now:**

```
Epoch 0:
  warmup_mult = 1/3 = 0.33
  scheduler LR = 1e-4
  actual LR = 1e-4 √ó 0.33 = 3.3e-5 ‚úì
  (Gentle start)

Epoch 1:
  warmup_mult = 2/3 = 0.67
  scheduler LR = 9.9e-5
  actual LR = 9.9e-5 √ó 0.67 = 6.6e-5 ‚úì
  (Building up)

Epoch 2:
  warmup_mult = 3/3 = 1.0
  scheduler LR = 9.8e-5
  actual LR = 9.8e-5 √ó 1.0 = 9.8e-5 ‚úì
  (Full speed)

Epoch 3+:
  warmup_mult = 1.0 (warmup done)
  actual LR = scheduler's LR ‚úì
  (Scheduler controls everything)
```

---

### **Fix #2: Lower Learning Rate** ‚úì

```python
'learning_rate': 1e-4,  # LOWERED from 3e-4
```

**Why this helps**:
- Smaller steps in loss landscape
- Won't overshoot minimum
- More stable convergence

**Trade-off**:
- Slower convergence (might need 10% more epochs)
- But WILL converge (unlike 3e-4 which diverges)

---

### **Fix #3: Disable Label Smoothing Initially** ‚úì

```python
'label_smoothing': 0.0,  # Disabled by default
```

Let model learn basics first. You can enable it later if needed:

```python
# After model reaches 80% accuracy, enable:
'label_smoothing': 0.1,
```

---

### **Fix #4: Divergence Detection** ‚úì (NEW!)

```python
'max_loss_increase': 0.5,  # Alert if loss increases by 0.5
'check_divergence_every_n_epochs': 2,

# In training loop:
if avg_loss > best_loss + CONFIG['max_loss_increase']:
    print(f"üö® WARNING: Loss increased from {best_loss:.4f} to {avg_loss:.4f}")
    print(f"Model may be diverging. Consider lowering learning rate.")
```

This will catch divergence early and warn you!

---

### **Bonus Fix: Data Augmentation** ‚úì (NEW!)

```python
'use_augmentation': True,  # 8√ó effective data

def augment_grid(grid):
    # Random rotation (0¬∞, 90¬∞, 180¬∞, 270¬∞)
    # Random horizontal/vertical flip
    # Random color permutation
    return augmented_grid
```

**Benefits**:
- 2,842 ‚Üí ~22,736 effective training samples
- Reduces overfitting
- Better generalization to test set
- Can support larger models if needed

**Example**:
```
Original:
[[1, 2],
 [3, 0]]

Augmented (rotate 90¬∞):
[[3, 1],
 [0, 2]]

Augmented (flip + swap colors):
[[0, 3],
 [2, 1]]
```

Model sees same pattern in different forms ‚Üí learns invariances!

---

## üìä Expected Behavior with v2

### **Healthy Training Loss**

```
Epoch 0: Loss = 2.30  ‚Üê Starts higher (random init)
Epoch 1: Loss = 1.85  ‚Üê Decreasing ‚úì
Epoch 2: Loss = 1.42  ‚Üê Decreasing ‚úì
Epoch 3: Loss = 1.15  ‚Üê Decreasing ‚úì
Epoch 5: Loss = 0.85  ‚Üê Decreasing ‚úì
Epoch 10: Loss = 0.52 ‚Üê Decreasing ‚úì
Epoch 20: Loss = 0.28 ‚Üê Decreasing ‚úì
Epoch 30: Loss = 0.19 ‚Üê Slowing down (converging)
Epoch 40: Loss = 0.16 ‚Üê Plateau (converged)
```

**Accuracy should track inversely**:
```
Epoch 0: Acc = 0.10  (10% - random)
Epoch 5: Acc = 0.42
Epoch 10: Acc = 0.65
Epoch 20: Acc = 0.82
Epoch 40: Acc = 0.88 (plateau)
```

---

## üÜö Comparison: v1 vs v2

| Aspect | v1 (DIVERGES) | v2 (FIXED) |
|--------|---------------|------------|
| **LR Schedule** | Warmup overwrites scheduler | Warmup works WITH scheduler |
| **Learning Rate** | 3e-4 (too high) | 1e-4 (stable) |
| **Label Smoothing** | 0.1 (confusing) | 0.0 (clear targets) |
| **Data Aug** | None | 8√ó effective data |
| **Divergence Check** | None | Auto-detection |
| **Expected Loss** | 0.6 ‚Üí 0.74 ‚úó | 2.3 ‚Üí 0.16 ‚úì |

---

## üéØ What You Should See Now

### **Training Output (v2)**:

```
================================================================================
OrcaSword v3 - ULTIMATE v2 (FIXED)
================================================================================

LOADING DATA
‚úì 400 training tasks
‚úì 240 test tasks
‚úì 2842 training pairs (FULL DATASET)
‚úì Data augmentation ENABLED (8√ó effective data)
  Effective training size: ~22,736 samples

MODEL INITIALIZATION
‚úì Total parameters: 2,457,610

TRAINING

Starting training at 10:30:45
Data augmentation: ENABLED

============================================================
Epoch 1/100 | Elapsed: 0:00:00
  Warmup: 0.33√ó | LR: 3.33e-05
============================================================
  Step  500/2842 | Loss=1.8543 | Acc=0.325 | Speed=45.2 samp/s | ETA=0:00:52
  Step 1000/2842 | Loss=1.6421 | Acc=0.412 | Speed=44.8 samp/s | ETA=0:00:41

  ‚úì Epoch 1 complete: Loss=1.5234, Acc=0.445
  üéØ New best loss: 1.5234
  üéØ New best accuracy: 0.445

============================================================
Epoch 2/100 | Elapsed: 0:01:03
  Warmup: 0.67√ó | LR: 6.66e-05
============================================================
  Step  500/2842 | Loss=1.2876 | Acc=0.521 | Speed=45.5 samp/s | ETA=0:00:51

  ‚úì Epoch 2 complete: Loss=1.1987, Acc=0.563
  üéØ New best loss: 1.1987
  üéØ New best accuracy: 0.563

============================================================
Epoch 3/100 | Elapsed: 0:02:06
  LR: 9.98e-05
============================================================

  ‚úì Epoch 3 complete: Loss=0.9543, Acc=0.645
  üéØ New best loss: 0.9543
  üéØ New best accuracy: 0.645

[... continues decreasing ...]

Epoch 40/100 | Elapsed: 1:42:00
  ‚úì Epoch 40 complete: Loss=0.1621, Acc=0.876

‚úì Training complete!
  Best loss: 0.1621
  Best accuracy: 0.876
```

**If you see this pattern ‚Üí Training is working! ‚úÖ**

**If loss still increases ‚Üí Try even lower LR (5e-5) or disable augmentation temporarily**

---

## üîß Quick Troubleshooting

### If loss STILL increases:

**Option 1: Lower LR further**
```python
'learning_rate': 5e-5,  # Even more conservative
```

**Option 2: Disable augmentation temporarily**
```python
'use_augmentation': False,  # Debug without aug first
```

**Option 3: Check your tweaks**
```python
# Did you change anything else?
# Reset to default v2 config and test
```

**Option 4: Gradient explosion check**
```python
# Add after loss.backward():
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        if grad_norm > 100:
            print(f"üö® Large gradient in {name}: {grad_norm}")
```

---

## üìå TL;DR

**Your Problem**: Loss 0.60 ‚Üí 0.74 (increasing)

**Root Causes**:
1. ‚ùå Warmup code overwrote scheduler (LR never decreased)
2. ‚ùå LR = 3e-4 too high (taking giant steps, overshooting)
3. ‚ùå Label smoothing confused early learning

**Solution**: Use `train_ULTIMATE_v2.py`
1. ‚úÖ Fixed warmup (epoch-based, works WITH scheduler)
2. ‚úÖ Lower LR (1e-4 for stability)
3. ‚úÖ No label smoothing initially
4. ‚úÖ Data augmentation (8√ó data)
5. ‚úÖ Divergence detection (warns you)

**Expected**: Loss 2.3 ‚Üí 1.5 ‚Üí 1.0 ‚Üí 0.5 ‚Üí 0.16 (DECREASING)

**Use train_ULTIMATE_v2.py now!** üöÄ
